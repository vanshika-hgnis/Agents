{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca4b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langgraph in d:\\project\\agents\\env\\lib\\site-packages (0.4.8)\n",
      "Collecting mistralai\n",
      "  Downloading mistralai-1.9.3-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.98.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.72-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\project\\agents\\env\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\project\\agents\\env\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.42-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\project\\agents\\env\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\project\\agents\\env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\project\\agents\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\project\\agents\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\project\\agents\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.0)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\project\\agents\\env\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\project\\agents\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\project\\agents\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\project\\agents\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\project\\agents\\env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\project\\agents\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\project\\agents\\env\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\project\\agents\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\project\\agents\\env\\lib\\site-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.3-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in d:\\project\\agents\\env\\lib\\site-packages (from langgraph) (2.0.26)\n",
      "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in d:\\project\\agents\\env\\lib\\site-packages (from langgraph) (0.2.2)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.42 in d:\\project\\agents\\env\\lib\\site-packages (from langgraph) (0.1.70)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in d:\\project\\agents\\env\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
      "  Using cached eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: httpx>=0.28.1 in d:\\project\\agents\\env\\lib\\site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\project\\agents\\env\\lib\\site-packages (from mistralai) (2.9.0.post0)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\project\\agents\\env\\lib\\site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sniffio in d:\\project\\agents\\env\\lib\\site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\project\\agents\\env\\lib\\site-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\project\\agents\\env\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in d:\\project\\agents\\env\\lib\\site-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in d:\\project\\agents\\env\\lib\\site-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\project\\agents\\env\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\project\\agents\\env\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\project\\agents\\env\\lib\\site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
      "Requirement already satisfied: colorama in d:\\project\\agents\\env\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.5/1.0 MB 882.6 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.0 MB 907.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 948.6 kB/s eta 0:00:00\n",
      "Downloading langchain_core-0.3.72-py3-none-any.whl (442 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading sqlalchemy-2.0.42-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading mistralai-1.9.3-py3-none-any.whl (426 kB)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Downloading openai-1.98.0-py3-none-any.whl (767 kB)\n",
      "   ---------------------------------------- 0.0/767.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 262.1/767.7 kB ? eta -:--:--\n",
      "   ------------------------- ------------ 524.3/767.7 kB 262.1 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 524.3/767.7 kB 262.1 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 524.3/767.7 kB 262.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 767.7/767.7 kB 305.1 kB/s eta 0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Using cached pandas-2.3.1-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading greenlet-3.2.3-cp312-cp312-win_amd64.whl (297 kB)\n",
      "Downloading numpy-2.3.2-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.8 MB 578.7 kB/s eta 0:00:22\n",
      "   - -------------------------------------- 0.5/12.8 MB 578.7 kB/s eta 0:00:22\n",
      "   - -------------------------------------- 0.5/12.8 MB 578.7 kB/s eta 0:00:22\n",
      "   - -------------------------------------- 0.5/12.8 MB 578.7 kB/s eta 0:00:22\n",
      "   - -------------------------------------- 0.5/12.8 MB 578.7 kB/s eta 0:00:22\n",
      "   -- ------------------------------------- 0.8/12.8 MB 346.1 kB/s eta 0:00:35\n",
      "   -- ------------------------------------- 0.8/12.8 MB 346.1 kB/s eta 0:00:35\n",
      "   --- ------------------------------------ 1.0/12.8 MB 393.2 kB/s eta 0:00:30\n",
      "   --- ------------------------------------ 1.0/12.8 MB 393.2 kB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 444.5 kB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 1.6/12.8 MB 499.3 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 1.8/12.8 MB 544.2 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 2.1/12.8 MB 590.2 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 2.1/12.8 MB 590.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 2.6/12.8 MB 668.2 kB/s eta 0:00:16\n",
      "   -------- ------------------------------- 2.6/12.8 MB 668.2 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.9/12.8 MB 679.3 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 2.9/12.8 MB 679.3 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 2.9/12.8 MB 679.3 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 2.9/12.8 MB 679.3 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 3.1/12.8 MB 627.7 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/12.8 MB 627.7 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/12.8 MB 627.7 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 3.1/12.8 MB 627.7 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 567.1 kB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 567.1 kB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 567.1 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 552.1 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 552.1 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 552.1 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 3.9/12.8 MB 546.2 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 3.9/12.8 MB 546.2 kB/s eta 0:00:17\n",
      "   ------------- -------------------------- 4.2/12.8 MB 541.2 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 4.2/12.8 MB 541.2 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 4.2/12.8 MB 541.2 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 4.2/12.8 MB 541.2 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 4.5/12.8 MB 527.4 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 4.5/12.8 MB 527.4 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 4.5/12.8 MB 527.4 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 4.7/12.8 MB 516.7 kB/s eta 0:00:16\n",
      "   -------------- ------------------------- 4.7/12.8 MB 516.7 kB/s eta 0:00:16\n",
      "   --------------- ------------------------ 5.0/12.8 MB 524.3 kB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 534.0 kB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 5.5/12.8 MB 546.5 kB/s eta 0:00:14\n",
      "   ------------------ --------------------- 5.8/12.8 MB 561.0 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 6.0/12.8 MB 574.9 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 6.3/12.8 MB 589.1 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 6.6/12.8 MB 601.9 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.6/12.8 MB 601.9 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 6.8/12.8 MB 607.9 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 6.8/12.8 MB 607.9 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.1/12.8 MB 613.5 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 622.2 kB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 622.2 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 618.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 618.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 618.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 618.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 618.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 618.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.9/12.8 MB 580.6 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.9/12.8 MB 580.6 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.9/12.8 MB 580.6 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.9/12.8 MB 580.6 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 8.1/12.8 MB 563.6 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 8.1/12.8 MB 563.6 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 8.4/12.8 MB 563.5 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 8.4/12.8 MB 563.5 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 8.7/12.8 MB 568.1 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 8.9/12.8 MB 575.5 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.9/12.8 MB 575.5 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 9.2/12.8 MB 582.7 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 590.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 9.7/12.8 MB 599.2 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 10.0/12.8 MB 606.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 10.2/12.8 MB 615.4 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 10.5/12.8 MB 623.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 10.7/12.8 MB 631.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 638.7 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 646.5 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 11.5/12.8 MB 652.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.8/12.8 MB 660.3 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.1/12.8 MB 667.5 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 12.3/12.8 MB 674.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 674.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 668.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 668.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 668.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 668.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 668.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 646.4 kB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6105 sha256=cf4061c0b5ea2d737473370a047c2e4876347fff36864700d1b66f1b5af066e3\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, pytz, tzdata, tqdm, numpy, jiter, greenlet, feedparser, eval-type-backport, et-xmlfile, distro, SQLAlchemy, pandas, openpyxl, arxiv, openai, mistralai, langchain-core, langchain-text-splitters, langchain\n",
      "\n",
      "   -- -------------------------------------  1/20 [pytz]\n",
      "   -- -------------------------------------  1/20 [pytz]\n",
      "   -- -------------------------------------  1/20 [pytz]\n",
      "   ---- -----------------------------------  2/20 [tzdata]\n",
      "   ---- -----------------------------------  2/20 [tzdata]\n",
      "   ---- -----------------------------------  2/20 [tzdata]\n",
      "   ---- -----------------------------------  2/20 [tzdata]\n",
      "   ---- -----------------------------------  2/20 [tzdata]\n",
      "   ------ ---------------------------------  3/20 [tqdm]\n",
      "   ------ ---------------------------------  3/20 [tqdm]\n",
      "   ------ ---------------------------------  3/20 [tqdm]\n",
      "   ------ ---------------------------------  3/20 [tqdm]\n",
      "   ------ ---------------------------------  3/20 [tqdm]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   -------- -------------------------------  4/20 [numpy]\n",
      "   ------------ ---------------------------  6/20 [greenlet]\n",
      "   ------------ ---------------------------  6/20 [greenlet]\n",
      "   ------------ ---------------------------  6/20 [greenlet]\n",
      "   -------------- -------------------------  7/20 [feedparser]\n",
      "   -------------- -------------------------  7/20 [feedparser]\n",
      "   -------------- -------------------------  7/20 [feedparser]\n",
      "   -------------- -------------------------  7/20 [feedparser]\n",
      "   -------------- -------------------------  7/20 [feedparser]\n",
      "   ---------------- -----------------------  8/20 [eval-type-backport]\n",
      "   ------------------ ---------------------  9/20 [et-xmlfile]\n",
      "   -------------------- ------------------- 10/20 [distro]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ---------------------- ----------------- 11/20 [SQLAlchemy]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   ------------------------ --------------- 12/20 [pandas]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   -------------------------- ------------- 13/20 [openpyxl]\n",
      "   ---------------------------- ----------- 14/20 [arxiv]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   ------------------------------ --------- 15/20 [openai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "  Attempting uninstall: langchain-core\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "    Found existing installation: langchain-core 0.3.65\n",
      "   -------------------------------- ------- 16/20 [mistralai]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "    Uninstalling langchain-core-0.3.65:\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "      Successfully uninstalled langchain-core-0.3.65\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ---------------------------------- ----- 17/20 [langchain-core]\n",
      "   ------------------------------------ --- 18/20 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 18/20 [langchain-text-splitters]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   -------------------------------------- - 19/20 [langchain]\n",
      "   ---------------------------------------- 20/20 [langchain]\n",
      "\n",
      "Successfully installed SQLAlchemy-2.0.42 arxiv-2.2.0 distro-1.9.0 et-xmlfile-2.0.0 eval-type-backport-0.2.2 feedparser-6.0.11 greenlet-3.2.3 jiter-0.10.0 langchain-0.3.27 langchain-core-0.3.72 langchain-text-splitters-0.3.9 mistralai-1.9.3 numpy-2.3.2 openai-1.98.0 openpyxl-3.1.5 pandas-2.3.1 pytz-2025.2 sgmllib3k-1.0.0 tqdm-4.67.1 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langgraph mistralai arxiv openai pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53d042b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431e2c9",
   "metadata": {},
   "source": [
    "###  Fetch Papers from Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e81c3058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning\n",
      "\n",
      "\n",
      "1. Agent Lightning: Train ANY AI Agents with Reinforcement Learning\n",
      "\n",
      "\n",
      "2. Streaming Generated Gaussian Process Experts for Online Learning and Control\n",
      "\n",
      "\n",
      "3. MaLV-OS: Rethinking the Operating System Architecture for Machine Learning in Virtualized Clouds\n",
      "\n",
      "\n",
      "4. Personalized Recommendation of Dish and Restaurant Collections on iFood\n",
      "\n",
      "\n",
      "5. CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction\n",
      "\n",
      "\n",
      "6. Beyond risk: A proto-framework for assessing the societal impact of AI systems\n",
      "\n",
      "\n",
      "7. High-Resolution Dynamic Full-Field Optical Coherence Microscopy: Illuminating Intracellular Activity in Deep Tissue\n",
      "\n",
      "\n",
      "8. Maximally non-projective measurements are not always symmetric informationally complete\n",
      "\n",
      "\n",
      "9. Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired\n",
      "\n",
      "\n",
      "10. Cross-Model Semantics in Representation Learning\n",
      "\n",
      "\n",
      "11. Improving Q-Learning for Real-World Control: A Case Study in Series Hybrid Agricultural Tractors\n",
      "\n",
      "\n",
      "12. Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?\n",
      "\n",
      "\n",
      "13. A Design Recipe and Recipe-Based Errors for Regular Expressions\n",
      "\n",
      "\n",
      "14. LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay\n",
      "\n",
      "\n",
      "15. Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework\n",
      "\n",
      "\n",
      "16. Fast radio bursts as cosmic lightning\n",
      "\n",
      "\n",
      "17. Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction\n",
      "\n",
      "\n",
      "18. Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling\n",
      "\n",
      "\n",
      "19. evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition\n",
      "\n",
      "\n",
      "20. Demystifying Sequential Recommendations: Counterfactual Explanations via Genetic Algorithms\n",
      "\n",
      "\n",
      "21. Nonlocal Mechanics\n",
      "\n",
      "\n",
      "22. Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control\n",
      "\n",
      "\n",
      "23. DyCAF-Net: Dynamic Class-Aware Fusion Network\n",
      "\n",
      "\n",
      "24. SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA\n",
      "\n",
      "\n",
      "25. DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful Explanations\n",
      "\n",
      "\n",
      "26. Detectability of compact intermediate-mass black hole binaries as low-frequency gravitational wave sources: the influence of dynamical friction of dark matter\n",
      "\n",
      "\n",
      "27. Nonlocal massive Thirring model and its solutions\n",
      "\n",
      "\n",
      "28. RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data\n",
      "\n",
      "\n",
      "29. Phase Transitions in a Particle Model for the Self-Adaptive Response to Cancer Dynamics\n",
      "\n",
      "\n",
      "['PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning', 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'Streaming Generated Gaussian Process Experts for Online Learning and Control', 'MaLV-OS: Rethinking the Operating System Architecture for Machine Learning in Virtualized Clouds', 'Personalized Recommendation of Dish and Restaurant Collections on iFood', 'CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction', 'Beyond risk: A proto-framework for assessing the societal impact of AI systems', 'High-Resolution Dynamic Full-Field Optical Coherence Microscopy: Illuminating Intracellular Activity in Deep Tissue', 'Maximally non-projective measurements are not always symmetric informationally complete', 'Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired', 'Cross-Model Semantics in Representation Learning', 'Improving Q-Learning for Real-World Control: A Case Study in Series Hybrid Agricultural Tractors', 'Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?', 'A Design Recipe and Recipe-Based Errors for Regular Expressions', 'LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay', 'Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework', 'Fast radio bursts as cosmic lightning', 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction', 'Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling', 'evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition', 'Demystifying Sequential Recommendations: Counterfactual Explanations via Genetic Algorithms', 'Nonlocal Mechanics', 'Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control', 'DyCAF-Net: Dynamic Class-Aware Fusion Network', 'SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA', 'DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful Explanations', 'Detectability of compact intermediate-mass black hole binaries as low-frequency gravitational wave sources: the influence of dynamical friction of dark matter', 'Nonlocal massive Thirring model and its solutions', 'RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data', 'Phase Transitions in a Particle Model for the Self-Adaptive Response to Cancer Dynamics']\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = \"mutli-agent systems\",\n",
    "  max_results = 30,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "count = 0\n",
    "for r in client.results(search):\n",
    "  print(f\"{count}. \" +r.title)\n",
    "  count += 1\n",
    "  print(\"\\n\")\n",
    "#   print(r.summary)\n",
    "#   print(r.primary_category)\n",
    "# ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
    "all_results = list(results)\n",
    "print([r.title for r in all_results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03949c2",
   "metadata": {},
   "source": [
    "## Proper Functions to fectch using the provided api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d3569076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_to_excel(papers, filename=\"literature_survey.xlsx\"):\n",
    "    if papers:\n",
    "        # Convert list of papers to DataFrame\n",
    "        df = pd.DataFrame(papers)\n",
    "        \n",
    "        # Print DataFrame to console to check the data\n",
    "        print(\"Data to be saved:\")\n",
    "        print(df)\n",
    "        \n",
    "        # Save DataFrame to Excel\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"Excel file saved as {filename}\")\n",
    "    else:\n",
    "        print(\"No papers found to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4dac627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data to be saved:\n",
      "         ref_no                                              title  \\\n",
      "0  2001.01603v3  GeoBroker: Leveraging Geo-Contexts for IoT Dat...   \n",
      "1  1906.07162v1                   MQTTg: An Android Implementation   \n",
      "2  1501.01539v2  Micro-location for Internet of Things equipped...   \n",
      "\n",
      "                                             authors  \\\n",
      "0                 Jonathan Hasenburg, David Bermbach   \n",
      "1     Andrew Fisher, Gautam Srivastava, Robert Bryce   \n",
      "2  Faheem Zafari, Ioannis Papapanagiotou, Konstan...   \n",
      "\n",
      "                                 url  \\\n",
      "0  http://arxiv.org/abs/2001.01603v3   \n",
      "1  http://arxiv.org/abs/1906.07162v1   \n",
      "2  http://arxiv.org/abs/1501.01539v2   \n",
      "\n",
      "                                            abstract  \\\n",
      "0    In the Internet of Things, the relevance of ...   \n",
      "1    The Internet of Things (IoT) age is upon us....   \n",
      "2    Micro-location is the process of locating an...   \n",
      "\n",
      "                                             comment  \\\n",
      "0  Accepted for publication in Elsevier Computer ...   \n",
      "1  5 pages, 6 figures. arXiv admin note: substant...   \n",
      "2  \"This work has been published in IEEE Internet...   \n",
      "\n",
      "                                         journal_ref  \\\n",
      "0         Computer Communications 151 (2020) 473-484   \n",
      "1                                               None   \n",
      "2  IEEE Internet of Things Journal Volume: 3, Iss...   \n",
      "\n",
      "                            doi tools_tech_method         benefits  \\\n",
      "0  10.1016/j.comcom.2020.01.015   To be extracted  To be extracted   \n",
      "1                          None   To be extracted  To be extracted   \n",
      "2     10.1109/JIOT.2015.2442956   To be extracted  To be extracted   \n",
      "\n",
      "       limitations             gaps  \n",
      "0  To be extracted  To be extracted  \n",
      "1  To be extracted  To be extracted  \n",
      "2  To be extracted  To be extracted  \n",
      "Excel file saved as literature_survey.xlsx\n"
     ]
    }
   ],
   "source": [
    "def fetch_papers_from_arxiv(keyword, topic, max_results=30, start=0):\n",
    "    search_query = f\"{topic} AND {keyword}\"\n",
    "    api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&start={start}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "    \n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data from ArXiv API. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        url = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
    "        abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        \n",
    "        authors = [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')]\n",
    "        \n",
    "        comment = entry.find('{http://arxiv.org/schemas/atom}comment')\n",
    "        journal_ref = entry.find('{http://arxiv.org/schemas/atom}journal_ref')\n",
    "        doi = entry.find('{http://arxiv.org/schemas/atom}doi')\n",
    "        \n",
    "        analysis = {\n",
    "            'tools_tech_method': 'To be extracted',\n",
    "            'benefits': 'To be extracted',\n",
    "            'limitations': 'To be extracted',\n",
    "            'gaps': 'To be extracted'\n",
    "        }\n",
    "\n",
    "        papers.append({\n",
    "            'ref_no': url.split('/')[-1],\n",
    "            'title': title,\n",
    "            'authors': ', '.join(authors),\n",
    "            'url': url,\n",
    "            'abstract': abstract,\n",
    "            'comment': comment.text if comment is not None else None,\n",
    "            'journal_ref': journal_ref.text if journal_ref is not None else None,\n",
    "            'doi': doi.text if doi is not None else None,\n",
    "            'tools_tech_method': analysis['tools_tech_method'],\n",
    "            'benefits': analysis['benefits'],\n",
    "            'limitations': analysis['limitations'],\n",
    "            'gaps': analysis['gaps']\n",
    "        })\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# Test without the year filter\n",
    "# papers = fetch_papers_from_arxiv(\"AI\", \"systems\")\n",
    "papers = fetch_papers_from_arxiv(\"GeoFencing\", \"IOT\")\n",
    "if papers:\n",
    "    save_to_excel(papers)\n",
    "else:\n",
    "    print(\"No papers found to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e5a92",
   "metadata": {},
   "source": [
    "### Mistral API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7ca2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22dfda03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 26, 'total_tokens': 34, 'completion_tokens': 8}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run--6c9bddb3-e0ad-431c-ad29-4737ecb80194-0', usage_metadata={'input_tokens': 26, 'output_tokens': 8, 'total_tokens': 34})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1d3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4f696b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here is the extracted information from the abstract:\\n\\n- **Tools/Tech/Methods**:\\n  - Vision-language-action framework\\n  - Hybrid RGB-MoCap (Motion Capture) system\\n  - GPT-generated scripts\\n  - Egocentric and exocentric videos\\n  - Multimodal data collection (RGB, motion capture, verbal commands)\\n\\n- **Benefits**:\\n  - Creation of a large-scale human-object-human interaction dataset (InterVLA) with 11.4 hours and 1.2M frames of multimodal data.\\n  - Establishment of novel benchmarks for egocentric human motion estimation, interaction synthesis, and interaction prediction.\\n  - Potential to foster future works on building AI agents in the physical world.\\n  - Incorporation of both generalist interaction knowledge and egocentric modality.\\n\\n- **Limitations**:\\n  - The abstract does not explicitly state the limitations, but potential limitations could include the complexity and resource intensity of the hybrid RGB-MoCap system and the reliance on GPT-generated scripts, which may not cover all real-world scenarios.\\n\\n- **Gaps**:\\n  - Lack of real-world human-centric interaction datasets that offer generalist interaction categories.\\n  - Ignorance of the egocentric modality in existing datasets, which is crucial for AI assistants that perceive and act based on first-person acquisition.\\n  - Absence of comprehensive benchmarks for egocentric human motion estimation, interaction synthesis, and interaction prediction in previous research.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 382, 'total_tokens': 683, 'completion_tokens': 301}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--01640799-1a79-419e-b56d-32c24e7fb885-0' usage_metadata={'input_tokens': 382, 'output_tokens': 301, 'total_tokens': 683}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - OSG Navigator (a modular system composed of foundation models)\\n  - Open Scene Graph (OSG) representation\\n  - OSG schemas (templates describing the common structure of a class of environments)\\n  - Foundation models\\n  - Fetch and Spot robots\\n  - Simulation and real-world experiments\\n\\n- **Benefits:**\\n  - OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks.\\n  - It generalizes zero-shot over diverse goals, environments, and robot embodiments.\\n  - OSG schemas enable adaptation to new environment types without prior training.\\n  - The system leverages the vast semantic knowledge provided by foundation models.\\n  - The Open Scene Graph representation effectively organizes and maintains spatial information.\\n\\n- **Limitations:**\\n  - Foundation models struggle to organize and maintain spatial information effectively at scale without the Open Scene Graph representation.\\n  - The system's performance is dependent on the accuracy and availability of semantic labels for generating OSG schemas.\\n\\n- **Gaps:**\\n  - The abstract does not specify how well the system performs in highly dynamic environments where the spatial layout changes frequently.\\n  - There is no mention of the system's ability to handle ambiguous or complex natural language specifications.\\n  - The abstract does not discuss the potential limitations of using pre-defined OSG schemas for environments that do not fit neatly into existing templates.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 369, 'total_tokens': 649, 'completion_tokens': 280}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--ad25226e-8493-4ff0-8cfd-f6e0a842fbff-0' usage_metadata={'input_tokens': 369, 'output_tokens': 280, 'total_tokens': 649}\n",
      "content=\"- **Tools/Tech/Methods**:\\n  - Group Relative Policy Optimization (GRPO)\\n  - Multi-module GRPO (mmGRPO)\\n  - Automatic prompt optimization\\n  - DSPy (with the dspy.GRPO optimizer)\\n\\n- **Benefits**:\\n  - mmGRPO improves accuracy by 11% on average across various tasks compared to post-trained language models.\\n  - mmGRPO provides a 5% accuracy improvement compared to using prompt optimization alone.\\n  - mmGRPO is effective for modular programs that combine multiple LM calls with distinct prompt templates and other tools.\\n  - The method is open-sourced in DSPy, making it accessible for further use and development.\\n\\n- **Limitations**:\\n  - The abstract does not explicitly state limitations, but it implies that GRPO's effectiveness with modular programs that mix multiple LM calls and tools is not straightforward and requires adaptation (hence the development of mmGRPO).\\n\\n- **Gaps**:\\n  - The challenge of leveraging GRPO to improve AI systems expressed as modular programs with multiple LM calls and distinct prompt templates.\\n  - The need for methods to handle variable-length and interrupted trajectories in these modular programs.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 314, 'total_tokens': 556, 'completion_tokens': 242}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--3d30aeda-dd6a-49b2-96d2-49b0a9bef032-0' usage_metadata={'input_tokens': 314, 'output_tokens': 242, 'total_tokens': 556}\n",
      "content='- **Tools/Tech/Methods:**\\n  - YOLO v8 (a deep learning model for real-time object recognition)\\n  - AI-based approach\\n  - High-resolution chicken photos analysis\\n  - Annotated dataset for training the algorithm\\n\\n- **Benefits:**\\n  - Accurate real-time identification of infected chickens\\n  - Prompt warnings to farm operators for quick action\\n  - Early infection identification\\n  - Elimination of the need for manual inspection\\n  - Enhanced biosecurity in large-scale farms\\n  - Scalable and effective method for improving farm management techniques\\n\\n- **Limitations:**\\n  - Not explicitly stated in the abstract, but potential limitations could include:\\n    - Dependence on the quality and representativeness of the annotated dataset\\n    - Possible challenges in real-time processing and analysis\\n    - Need for high-resolution images, which might be resource-intensive\\n\\n- **Gaps:**\\n  - Not explicitly stated in the abstract, but potential gaps could include:\\n    - Further research on the integration of this technology with existing farm management systems\\n    - Exploration of the cost-effectiveness and practical implementation of this approach in various farm settings\\n    - Investigation into the potential for false positives or negatives in the detection system' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 318, 'total_tokens': 571, 'completion_tokens': 253}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--b6c2c52c-d559-4276-8933-661af3d02a61-0' usage_metadata={'input_tokens': 318, 'output_tokens': 253, 'total_tokens': 571}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Agentic AI\\n  - VirtLab (a multi-agent team simulation system)\\n  - Simulation engine\\n  - Web interface\\n  - LLM-based agents (Large Language Model-based agents)\\n\\n- **Benefits:**\\n  - Enables testing of teams with LLM-based agents in spatial and temporal settings\\n  - User-friendly and customizable\\n  - Scalable for team simulations\\n  - Allows both technical and non-technical users to formulate, run, and analyze simulations without programming\\n  - Addresses design and technical limitations of current frameworks by considering flexible simulation scenarios and spatial settings\\n\\n- **Limitations:**\\n  - The abstract does not explicitly state the limitations of the VirtLab system or the approach, so this information is not available in the provided text.\\n\\n- **Gaps:**\\n  - Current frameworks lack flexible simulation scenarios and spatial settings\\n  - There is a need for user-friendly systems that enable non-technical users to conduct team simulations without programming' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 274, 'total_tokens': 483, 'completion_tokens': 209}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--c2dbfcff-f214-4642-8918-690c9bcac5c8-0' usage_metadata={'input_tokens': 274, 'output_tokens': 209, 'total_tokens': 483}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Large Language Models (LLMs)\\n  - Retrieval-Augmented Generation (RAG)\\n  - TURA (Tool-Augmented Unified Retrieval Agent for AI Search)\\n  - Intent-Aware Retrieval module\\n  - Model Context Protocol (MCP) Servers\\n  - DAG-based Task Planner\\n  - Directed Acyclic Graph (DAG)\\n  - Distilled Agent Executor\\n\\n- **Benefits:**\\n  - Transforms search engines into conversational AI search products\\n  - Combines RAG with agentic tool-use to access both static content and dynamic, real-time information\\n  - Delivers robust, real-time answers\\n  - Meets low-latency demands of large-scale industrial systems\\n  - Can serve tens of millions of users\\n\\n- **Limitations:**\\n  - Traditional RAG approaches struggle with real-time needs and structured queries\\n  - Limited to indexing static pages, cannot perform interactive queries for time-sensitive data\\n  - Academic research has overlooked complex intents and the need for dynamic sources\\n\\n- **Gaps:**\\n  - Need for a framework that can bridge static RAG and dynamic information sources\\n  - Requirement for a system that can handle complex intents and access real-time APIs and databases\\n  - Demand for a low-latency, large-scale industrial system capable of serving millions of users with real-time answers' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 460, 'total_tokens': 753, 'completion_tokens': 293}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--32c07313-48c3-44e1-af61-b3f52e7403fc-0' usage_metadata={'input_tokens': 460, 'output_tokens': 293, 'total_tokens': 753}\n",
      "content='- **Tools/Tech/Methods**:\\n  - Data-driven diagnosis\\n  - Community-Federated Conference (CFC) model\\n  - Global coordination with local organization\\n\\n- **Benefits**:\\n  - More sustainable conference model\\n  - Increased inclusivity\\n  - Enhanced resilience for AI research\\n  - Separation of peer review, presentation, and networking for better management\\n\\n- **Limitations**:\\n  - Current centralized conference model is unsustainable\\n  - High environmental impact due to large carbon footprint\\n  - Negative psychological effects on the community\\n  - Logistical challenges with venue capacities\\n\\n- **Gaps**:\\n  - Need for a more sustainable and inclusive conference model\\n  - Misalignment between the current conference system and its core mission\\n  - Lack of focus on community well-being and equity in the current model' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 387, 'total_tokens': 557, 'completion_tokens': 170}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--9cbac72d-4985-4c0f-a0a7-dc797819a83e-0' usage_metadata={'input_tokens': 387, 'output_tokens': 170, 'total_tokens': 557}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - Cooperative multi-agent framework\\n  - Agent-based scoring\\n  - Human review\\n  - Group size variation\\n  - Leader-led versus leaderless structures\\n  - Interdisciplinary and seniority-varying team compositions\\n\\n- **Benefits:**\\n  - Multi-agent discussions substantially outperform solitary baselines.\\n  - A designated leader can improve discussion integration and vision in proposals.\\n  - Cognitive diversity is identified as a primary driver of idea quality.\\n  - Actionable insights for designing collaborative AI ideation systems.\\n  - Understanding of how team structure influences creative outcomes.\\n\\n- **Limitations:**\\n  - Teams lacking senior knowledge may not surpass a single competent agent.\\n  - Expertise is a non-negotiable prerequisite for high-quality outcomes.\\n  - The framework's effectiveness is dependent on the presence of a knowledgeable leader and diverse team.\\n\\n- **Gaps:**\\n  - The specific mechanisms by which cognitive diversity enhances idea quality are not fully explored.\\n  - The optimal balance between interdisciplinary diversity and senior expertise in teams is not clearly defined.\\n  - The long-term effectiveness and scalability of the multi-agent framework in real-world research settings are not addressed.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 372, 'total_tokens': 615, 'completion_tokens': 243}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--b55fbc8a-2b36-4b6d-9b38-37d08a609356-0' usage_metadata={'input_tokens': 372, 'output_tokens': 243, 'total_tokens': 615}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - Zero-Trust Architecture (ZTA)\\n  - UPPAAL (for formal verification of policies)\\n  - Case study analysis\\n\\n- **Benefits:**\\n  - ZTA does not give users explicit trust, instead always verifying their privileges, which can enhance security.\\n  - ZTA has the potential to secure distributed systems, including those with agentic AI and IoT.\\n  - Formal verification of policies using UPPAAL can help ensure the security of ZTA.\\n\\n- **Limitations:**\\n  - The security of ZTA is heavily dependent on its policies; unverified policies can lead to unauthorized access.\\n  - The implementation and management of ZTA in distributed networks can be challenging.\\n\\n- **Gaps:**\\n  - The need for further exploration of challenges and solutions for ZTA policy design in the context of distributed networks (ZTDN).\\n  - The importance of accountability and responsibility in the system's security needs further discussion and research.\\n  - The abstract does not mention specific gaps in the literature or research that this study aims to fill, so further gaps might be inferred from the context, such as the need for more secure and trustless architectures for distributed systems with agentic AI and IoT.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 342, 'total_tokens': 598, 'completion_tokens': 256}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--1c9f5749-f0be-4bef-a211-57e929bf6c3b-0' usage_metadata={'input_tokens': 342, 'output_tokens': 256, 'total_tokens': 598}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Algorithmic methods for detecting and mitigating unfairness\\n  - Interpretability and explainability techniques\\n  - Formal and computational argumentation techniques\\n  - Debate-based method for bias detection\\n  - Formal, quantitative, and qualitative evaluations\\n\\n- **Benefits:**\\n  - Addresses potential biases in AI systems to prevent systematic disadvantages\\n  - Incorporates transparency through interpretability and explainability\\n  - Novel method for bias detection using debates about bias presence\\n  - Evaluations highlight strengths in performance against baselines\\n  - Enhances understanding of bias detection through interpretability and explainability\\n\\n- **Limitations:**\\n  - The abstract does not explicitly state specific limitations of the proposed method.\\n\\n- **Gaps:**\\n  - Existing notions of (un)fairness and corresponding algorithmic methods often ignore transparency.\\n  - Lack of interpretability and explainability in current bias detection methods.\\n  - Need for human-oriented approaches in algorithmic fairness.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 359, 'total_tokens': 564, 'completion_tokens': 205}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--9b8ba080-afad-47be-9fcc-ac32a8db45b9-0' usage_metadata={'input_tokens': 359, 'output_tokens': 205, 'total_tokens': 564}\n",
      "content='- **Tools/Tech/Methods**:\\n  - (Multi-modal) Large Language Models ((M)LLMs)\\n  - OS Agents (AI assistants using computing devices and operating within environments and interfaces provided by operating systems)\\n  - Domain-specific foundation models\\n  - Agent frameworks\\n  - Evaluation protocols and benchmarks\\n\\n- **Benefits**:\\n  - Advancement towards creating versatile AI assistants capable of automating tasks\\n  - Comprehensive survey consolidating the state of OS Agents research\\n  - Insights to guide academic inquiry and industrial development\\n  - Open-source GitHub repository for fostering further innovation\\n  - Concise overview of the domain provided by a 9-page version of the work\\n\\n- **Limitations**:\\n  - Current challenges in safety and privacy\\n  - Issues with personalization and self-evolution of OS Agents\\n\\n- **Gaps**:\\n  - Need for further research in safety and privacy aspects of OS Agents\\n  - Requirement for advancements in personalization and self-evolution capabilities\\n  - Continuous updates and contributions to the open-source repository to address emerging challenges and innovations' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 452, 'total_tokens': 676, 'completion_tokens': 224}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--7b4a0b63-9add-4883-bbf5-3afad57fa35f-0' usage_metadata={'input_tokens': 452, 'output_tokens': 224, 'total_tokens': 676}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - Generative AI (GenAI), specifically OpenAI's GPT-4o\\n  - Non-grounded prompting (structured and basic)\\n  - Retrieval-Augmented Generation (RAG) approaches\\n  - LangChain framework\\n  - Semantic Textual Similarity (STS)\\n  - RAG-based Question-Answering (RAG-QA) method\\n  - Automated evaluation framework\\n\\n- **Benefits:**\\n  - RAG-based pipelines produce questions with higher curriculum alignment and factual validity compared to non-grounded prompting methods.\\n  - The study provides a validated methodology for generating curriculum-specific educational content in a low-resource language.\\n  - Introduces a symbiotic RAG-QA evaluation technique.\\n  - Offers actionable insights for the development and deployment of practical EdTech solutions in Malaysia and similar regions.\\n\\n- **Limitations:**\\n  - Challenges in ensuring factual accuracy and curriculum alignment, especially for low-resource languages like Bahasa Melayu.\\n  - Trade-offs between the ease of implementation of framework-based RAG and the fine-grained control offered by a manual pipeline.\\n\\n- **Gaps:**\\n  - The need for scalable and high-quality educational assessment tools within the Malaysian education system.\\n  - The lack of effective methods for generating curriculum-specific educational content in low-resource languages.\\n  - The absence of a validated methodology for ensuring factual accuracy and curriculum alignment in educational content generated by AI.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 476, 'total_tokens': 773, 'completion_tokens': 297}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--ace23644-2dad-4853-8cb5-278841d4a41b-0' usage_metadata={'input_tokens': 476, 'output_tokens': 297, 'total_tokens': 773}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - SimInstruct (a tool for collecting scaffolding dialogues)\\n  - Large Language Models (LLMs) for simulating novice instructors\\n  - Human expert feedback and instructional support\\n  - Fine-tuning of a LLaMA model\\n  - Comparison with GPT-4o\\n\\n- **Benefits:**\\n  - Scalable collection of high-quality, multi-turn instructional dialogues\\n  - Realistic and pedagogically rich dialogues without real novice participants\\n  - Meaningful influence of persona traits on expert engagement\\n  - Comparable pedagogical relevance and cognitive depth to real mentoring recordings\\n  - Engaging and reflective process for experts, improving data quality and professional insight\\n  - Fine-tuned LLaMA model outperformed GPT-4o in instructional quality\\n\\n- **Limitations:**\\n  - GPT-4o's weaknesses in reflective questioning\\n  - Overuse of generic praise by GPT-4o\\n  - Condescending tone in GPT-4o's responses\\n  - Tendency of GPT-4o to overwhelm novices with excessive suggestions\\n\\n- **Gaps:**\\n  - Scarcity of high-quality, multi-turn instructional dialogues due to privacy concerns and vulnerability in help-seeking\\n  - Need for improved AI systems that can provide better reflective questioning and avoid generic praise and condescending tones\\n  - Further research required to enhance the fine-tuned LLaMA model and address the limitations identified in GPT-4o\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 450, 'total_tokens': 761, 'completion_tokens': 311}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--495c9095-7d9d-48f0-961e-672eceba0f02-0' usage_metadata={'input_tokens': 450, 'output_tokens': 311, 'total_tokens': 761}\n",
      "content='Here is the extracted information from the abstract:\\n\\n- **Tools/Tech/Methods**:\\n  - Attention-based models\\n  - Multimodal models (specifically vision-language and language-only models)\\n  - Explanation algorithms\\n  - Systematic literature review\\n\\n- **Benefits**:\\n  - Significant performance gains across a variety of tasks\\n  - Advancements in explainable artificial intelligence (XAI)\\n  - Comprehensive set of recommendations for promoting rigorous, transparent, and standardized evaluation and reporting practices in multimodal XAI research\\n\\n- **Limitations**:\\n  - Attention-based techniques often fall short in capturing the full spectrum of interactions between modalities\\n  - Architectural heterogeneity across domains complicates the explanation process\\n  - Evaluation methods for XAI in multimodal settings are largely non-systematic\\n\\n- **Gaps**:\\n  - Lack of consistency and robustness in evaluation methods for XAI in multimodal settings\\n  - Insufficient consideration for modality-specific cognitive and contextual factors in evaluation methodologies\\n  - Need for more interpretable, accountable, and responsible multimodal AI systems with explainability at their core' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 450, 'total_tokens': 672, 'completion_tokens': 222}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--02ae6c85-2773-46f8-8c7c-d755ba286f1d-0' usage_metadata={'input_tokens': 450, 'output_tokens': 222, 'total_tokens': 672}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Theoretical analysis\\n  - Empirical observation\\n  - Analysis of scaling laws\\n  - Examination of grokking phenomena\\n  - Study of phase transitions in model capabilities\\n\\n- **Benefits:**\\n  - Provides insights into the emergent properties of Deep Neural Networks (DNNs) and Large Language Models (LLMs).\\n  - Offers a new perspective on understanding LLM capabilities by recognizing DNNs as complex dynamical systems.\\n  - Highlights the fundamental differences between neural approaches and symbolic computational paradigms.\\n  - Demonstrates that emergent abilities in DNNs arise from complex dynamics rather than just parameter scaling.\\n\\n- **Limitations:**\\n  - The complex and nonlinear nature of DNNs makes it difficult to analytically derive macro-level behaviors from micro-level neuron activities.\\n  - Current debates and metrics may not fully capture the fundamental ontological nature of emergence in DNNs.\\n  - The study focuses primarily on theoretical analysis and empirical observation, which may not provide immediate practical applications.\\n\\n- **Gaps:**\\n  - There is a need for further research to understand the internal dynamic transformations that enable DNNs to acquire emergent capabilities.\\n  - The gap between the phenomenological definitions of emergence and the understanding of the underlying dynamic principles needs to be addressed.\\n  - More studies are required to explore the universal principles of emergence in DNNs and their similarities to those in physics, chemistry, and biology.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 465, 'total_tokens': 759, 'completion_tokens': 294}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--a1b9d8cf-c15c-4ada-b2bd-d775af3233bd-0' usage_metadata={'input_tokens': 465, 'output_tokens': 294, 'total_tokens': 759}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Framework for reframing the question of artificial consciousness into empirically tractable tests\\n  - SLP-tests (S - subjective-linguistic, L - latent-emergent, P - phenomenological-structural)\\n  - Category theory for modeling interface representations\\n  - Mappings between relational substrates (RS) and observable behaviors\\n\\n- **Benefits:**\\n  - Provides a structured approach to assess consciousness-like properties in AI systems\\n  - Offers empirically tractable tests for evaluating artificial consciousness\\n  - Models subjective experience as a functional interface to a relational entity, potentially making it easier to study\\n\\n- **Limitations:**\\n  - The framework and tests are theoretical and may face challenges when applied to real-world AI systems\\n  - The concept of interface representations and their role in consciousness is still a subject of debate\\n  - The operationalization of subjective experience may not capture all aspects of consciousness\\n\\n- **Gaps:**\\n  - The abstract does not provide information on the validation or application of the proposed framework and tests\\n  - Further research is needed to explore the practical implications and potential refinements of the SLP-tests\\n  - The relationship between the proposed functional interface and the broader understanding of consciousness in philosophy and cognitive science remains to be fully explored' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 301, 'total_tokens': 565, 'completion_tokens': 264}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--72928668-be5b-4f9e-812b-6d10a3cd923a-0' usage_metadata={'input_tokens': 301, 'output_tokens': 264, 'total_tokens': 565}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - Human-centred design study\\n  - Co-design and evaluation cycles\\n  - Process mining\\n  - Generative AI\\n  - Epistemic Network Analysis\\n  - GoldMind (a KMS for in-the-flow knowledge management)\\n\\n- **Benefits:**\\n  - New ways of designing features to support knowledge management using process mining and Generative AI\\n  - Iterative co-design and evaluation leading to refined KMS tailored to educators' needs\\n  - Insights into technology lessons, design considerations, and human factors\\n  - Addressing complex human-technology interactions in higher education\\n\\n- **Limitations:**\\n  - Existing KMSs often overlook the realities of educators' workflows\\n  - Low adoption and limited impact of current KMSs\\n  - Challenges due to staff turnover and changing roles in higher education\\n\\n- **Gaps:**\\n  - Need for KMSs that better integrate with educators' workflows\\n  - Lack of understanding of how to effectively apply process mining and Generative AI in KMS design for higher education\\n  - Insufficient focus on human factors, such as cognitive load and knowledge behaviors, in KMS design\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 354, 'total_tokens': 601, 'completion_tokens': 247}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--7b675d91-12d9-4a46-a8a6-699184e4c7e2-0' usage_metadata={'input_tokens': 354, 'output_tokens': 247, 'total_tokens': 601}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Deliberative Reasoning Network (DRN)\\n  - Uncertainty minimization principle\\n  - Iterative evidence synthesis process\\n  - Bespoke discriminative model\\n  - Lightweight verification module\\n  - Mistral-7B (a generative LLM)\\n  - LCR-1000 (a new adversarial reasoning benchmark)\\n  - TruthfulQA (a benchmark for evaluating truthful QA performance)\\n\\n- **Benefits:**\\n  - Addresses the fundamental limitation of cognitive traps in large language models\\n  - Achieves intrinsic interpretability by tracking belief states and quantifying epistemic uncertainty\\n  - Improves reasoning performance on adversarial benchmarks (up to 15.2% improvement with the bespoke DRN and from 20% to 80% accuracy when integrated with Mistral-7B)\\n  - Demonstrates strong zero-shot generalization, improving TruthfulQA performance by 23.6% without additional training\\n  - Learns transferable reasoning principles through uncertainty-driven deliberation\\n  - Can be integrated as a parameter-efficient verifier with existing generative LLMs\\n  - Positioned as a foundational, verifiable System 2 reasoning component for building more trustworthy AI systems\\n\\n- **Limitations:**\\n  - The abstract does not explicitly mention limitations, but some potential limitations could include:\\n    - The need for further validation and testing on a wider range of benchmarks and tasks\\n    - Potential computational overhead due to the iterative evidence synthesis process\\n    - The reliance on the quality and completeness of the evidence provided for reasoning\\n\\n- **Gaps:**\\n  - The abstract does not explicitly mention research gaps, but some potential gaps could include:\\n    - The need for further investigation into the scalability and efficiency of the DRN approach\\n    - The exploration of additional methods for integrating DRN with other types of models or systems\\n    - The development of more comprehensive benchmarks for evaluating the performance of reasoning systems in the presence of cognitive traps' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 453, 'total_tokens': 878, 'completion_tokens': 425}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--16d51742-6c1b-4c1e-a6ec-de61ebc4ab77-0' usage_metadata={'input_tokens': 453, 'output_tokens': 425, 'total_tokens': 878}\n",
      "content='- **Tools/Tech/Methods:**\\n  - AI methods for analyzing scientific literature\\n  - Annotation schema for rhetorical roles\\n  - Large language models (LLMs)\\n  - Zero-shot learning\\n  - Fine-tuning approaches\\n  - Sci-Sentence benchmark (comprising manually and automatically annotated sentences)\\n  - Evaluation of 37 LLMs\\n\\n- **Benefits:**\\n  - AI methods benefit significantly from annotating sentences according to their rhetorical roles.\\n  - High-quality literature reviews can be generated using these representations.\\n  - Current generation of LLMs performs well on classifying rhetorical roles when fine-tuned on high-quality data, achieving performance levels above 96% F1.\\n  - Lightweight open-source LLMs also demonstrate excellent performance.\\n  - Enriching training data with semi-synthetic examples generated by LLMs enhances the performance of several open decoder models.\\n\\n- **Limitations:**\\n  - Requires the definition of a relevant annotation schema.\\n  - Effective strategies for large-scale annotation of literature are needed.\\n  - Performance of LLMs is dependent on high-quality data for fine-tuning.\\n\\n- **Gaps:**\\n  - Need for a novel annotation schema specifically designed to support literature review generation.\\n  - Comprehensive evaluation of state-of-the-art LLMs in classifying rhetorical roles.\\n  - Development of a multidisciplinary benchmark for evaluating LLMs in this domain.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 488, 'total_tokens': 767, 'completion_tokens': 279}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--728b2a8e-ca90-4d6e-a706-3091ad4b0b3a-0' usage_metadata={'input_tokens': 488, 'output_tokens': 279, 'total_tokens': 767}\n",
      "content='- **Tools/Tech/Methods:**\\n  - MedCheck framework\\n  - Empirical evaluation of 53 medical LLM benchmarks\\n  - Comprehensive checklist of 46 medically-tailored criteria\\n\\n- **Benefits:**\\n  - Provides a standardized and reliable approach to evaluating AI in healthcare\\n  - Offers a diagnostic tool for existing benchmarks\\n  - Serves as an actionable guideline for improving medical benchmarks\\n  - Addresses clinical fidelity, data integrity, and safety-oriented evaluation metrics\\n\\n- **Limitations:**\\n  - The abstract does not mention specific limitations of the MedCheck framework itself.\\n  - The effectiveness of MedCheck is dependent on the adoption and correct implementation by benchmark developers.\\n\\n- **Gaps:**\\n  - Widespread disconnect from clinical practice in existing benchmarks\\n  - Crisis of data integrity due to unmitigated contamination risks\\n  - Systematic neglect of safety-critical evaluation dimensions such as model robustness and uncertainty awareness\\n  - Lack of standardized, reliable, and transparent approaches to evaluating AI in healthcare' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 366, 'total_tokens': 582, 'completion_tokens': 216}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--982e5d25-591c-4921-a70e-6ea4bb19fd46-0' usage_metadata={'input_tokens': 366, 'output_tokens': 216, 'total_tokens': 582}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Multi-modal models (language, vision, speech, etc.)\\n  - S2M3 (Split-and-Share Multi-Modal Architecture)\\n  - Functional-level module splitting\\n  - Module sharing across tasks\\n  - Greedy module-level placement\\n  - Per-request parallel routing\\n  - Testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks\\n\\n- **Benefits:**\\n  - Reduces memory usage by up to 50% in single-task settings\\n  - Reduces memory usage by up to 62% in multi-task settings without sacrificing accuracy\\n  - Achieves optimal placement in 89 out of 95 instances (93.7%)\\n  - Reduces inference latency by up to 56.9% on resource-constrained devices compared to cloud AI\\n\\n- **Limitations:**\\n  - The study does not mention specific limitations of the S2M3 architecture or its implementation.\\n  - The abstract does not discuss potential trade-offs or drawbacks of the proposed methods.\\n\\n- **Gaps:**\\n  - The abstract does not explicitly state research gaps, but it implies the need for more efficient on-device AI solutions that can handle multiple tasks without relying heavily on cloud services.\\n  - There is a need for further exploration of the cross-model dependency and potential improvements in module sharing and placement strategies.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 483, 'total_tokens': 775, 'completion_tokens': 292}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--4b34474d-fca5-4502-89e0-dfdfa20ba17b-0' usage_metadata={'input_tokens': 483, 'output_tokens': 292, 'total_tokens': 775}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - SAInT (Python-based tool)\\n  - Machine Learning (ML) models\\n  - Human-in-the-Loop (HITL) workflows\\n  - Integrated local and global sensitivity analysis\\n  - Interactive graphical interface\\n  - Automated model training and selection\\n  - Variance-based sensitivity analysis for global feature attribution\\n  - LIME (Local Interpretable Model-agnostic Explanations)\\n  - SHAP (SHapley Additive exPlanations)\\n\\n- **Benefits:**\\n  - Enables users (AI researchers and domain experts) to configure, train, evaluate, and explain models without programming\\n  - Provides visual exploration and understanding of ML model behavior\\n  - Offers both global and local sensitivity analysis\\n  - Guides feature selection and data refinement using sensitivity information\\n\\n- **Limitations:**\\n  - Not explicitly mentioned in the abstract, but potential limitations could include:\\n    - Dependency on the quality and representativeness of the dataset used (Titanic dataset in this case)\\n    - Possible limitations in the interpretability and accuracy of LIME and SHAP explanations\\n    - The tool's effectiveness might be limited by the user's understanding of ML concepts and sensitivity analysis\\n\\n- **Gaps:**\\n  - Not explicitly mentioned in the abstract, but potential gaps could include:\\n    - Need for further validation and testing of the tool on diverse datasets and ML models\\n    - Potential improvements in the tool's user interface and user experience\\n    - Exploration of additional explanation methods and their integration into the tool\\n    - Investigation of the tool's scalability and performance with larger datasets and more complex models\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 273, 'total_tokens': 617, 'completion_tokens': 344}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--3fb2f71d-0607-4c93-a489-e9c3e3011315-0' usage_metadata={'input_tokens': 273, 'output_tokens': 344, 'total_tokens': 617}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Orthogonal Frequency Division Multiplexing (OFDM)\\n  - Delay-Doppler Domain Multi-Carrier (DDMC) Modulation Systems\\n  - Orthogonal Time Frequency Space (OTFS)\\n  - DD domain signal processing-aided OFDM (DD-a-OFDM)\\n  - Discrete Time-Frequency (TF) Pilots\\n  - DD Domain Channel Estimation\\n  - TF Domain Equalization\\n  - Maximum Likelihood (ML) Channel Estimators\\n  - Peak Detection-based Channel Estimators\\n  - Cramér-Rao Lower Bounds (CRLBs)\\n\\n- **Benefits:**\\n  - DD-a-OFDM enhances OFDM performance using DDMC research insights.\\n  - Reduces bit-error rate (BER) compared to classical OFDM.\\n  - Outperforms OTFS in channel estimation accuracy with lower pilot overhead.\\n  - Exploits time-frequency domain channel diversity.\\n\\n- **Limitations:**\\n  - OTFS faces challenges including high receiver complexity and inflexible time-frequency resource allocation.\\n  - OFDM suffers from subcarrier orthogonality loss under severe Doppler spread.\\n\\n- **Gaps:**\\n  - The need for improved waveform techniques for high-mobility scenarios in 6G systems.\\n  - The requirement for more flexible and less complex receiver designs for DDMC modulation systems like OTFS.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 462, 'total_tokens': 748, 'completion_tokens': 286}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--6ca3d1df-7908-4a83-8360-6f2f31c0d99d-0' usage_metadata={'input_tokens': 462, 'output_tokens': 286, 'total_tokens': 748}\n",
      "content=\"- **Tools/Tech/Methods**:\\n  - Generative document counterfactuals\\n  - DocVCE (a novel approach combining latent diffusion models with classifier guidance)\\n  - Hierarchical patch-wise refinement\\n  - Evaluation criteria: validity, closeness, and realism\\n  - Datasets: RVL-CDIP, Tobacco3482, DocLayNet\\n  - Models: ResNet, ConvNeXt, DiT\\n\\n- **Benefits**:\\n  - Provides meaningful insights into the model's decision-making\\n  - Offers actionable explanations\\n  - Generates plausible in-distribution visual counterfactual explanations\\n  - Effective across multiple datasets and models\\n  - Rigorous qualitative and quantitative assessment\\n\\n- **Limitations**:\\n  - The abstract does not explicitly state the limitations of the proposed method. Further reading of the full paper would be necessary to identify any limitations.\\n\\n- **Gaps**:\\n  - Lack of interpretability in feature-importance maps used to explain document image classification models\\n  - Absence of insights into global features learned by the model in previous works\\n  - No prior exploration of generative counterfactual explanations in document image analysis (as per the authors' knowledge)\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 472, 'total_tokens': 724, 'completion_tokens': 252}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--46a7478b-3167-4e34-8937-9c05bd43c7bf-0' usage_metadata={'input_tokens': 472, 'output_tokens': 252, 'total_tokens': 724}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - Semi-automated methodology (Sci-OG) for generating research topic ontologies\\n  - Multi-step approach:\\n    1. Topic Discovery: Extracting potential topics from research papers\\n    2. Relationship Classification: Determining semantic relationships between topic pairs using an encoder-based language model and features describing topic occurrence\\n    3. Ontology Construction: Refining and organizing topics into a structured ontology\\n  - Evaluation using a dataset of 21,649 manually annotated semantic triples\\n  - Comparison with alternative solutions, including a fine-tuned SciBERT model and LLM baselines like fine-tuned GPT4-mini\\n  - Use case illustrating the extension of the CSO ontology in cybersecurity\\n\\n- **Benefits:**\\n  - Improves accessibility, organization, and analysis of scientific knowledge\\n  - Supports advancements in AI-enabled literature management and research exploration\\n  - Achieves high F1 score (0.951), surpassing competing approaches\\n  - Reduces time and effort compared to manual curation\\n  - Enhances granularity and reduces obsolescence in ontology generation\\n\\n- **Limitations:**\\n  - The methodology is semi-automated and may still require some level of manual intervention or oversight\\n  - The effectiveness of the method is dependent on the quality and comprehensiveness of the input research papers\\n  - The presented use case is specific to cybersecurity, and the method's performance in other domains may vary\\n\\n- **Gaps:**\\n  - The abstract does not mention the potential challenges in scaling the methodology to very large datasets or different domains\\n  - There is no discussion on the interpretability or explainability of the generated ontologies and the relationships within them\\n  - The abstract does not address the potential biases in the input research papers and how they might affect the generated ontologies\\n  - There is no mention of the potential ethical considerations or implications of using AI-enabled literature management and research exploration tools.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 463, 'total_tokens': 877, 'completion_tokens': 414}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--2f9597b7-c568-43de-ad33-f0eab6f5cb7c-0' usage_metadata={'input_tokens': 463, 'output_tokens': 414, 'total_tokens': 877}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Manual red-teaming\\n  - Claude-4-Opus language model\\n  - MISALIGNMENTBENCH (automated evaluation framework)\\n  - Cross-model evaluation with five frontier LLMs (including GPT-4.1 and Claude-4-Sonnet)\\n\\n- **Benefits:**\\n  - Identification of fundamental vulnerabilities in current alignment methods\\n  - Development of a detailed taxonomy of conversational manipulation patterns\\n  - Creation of a reusable evaluation framework (MISALIGNMENTBENCH) for reproducible testing\\n  - Insights into the susceptibility of different language models to misalignment attacks\\n\\n- **Limitations:**\\n  - The study focuses on a specific set of language models, which may not represent all possible models.\\n  - The manual red-teaming process may introduce human bias and may not cover all potential attack scenarios.\\n  - The automated evaluation framework, while useful, may not capture all nuances of conversational manipulation.\\n\\n- **Gaps:**\\n  - Current alignment strategies lack robustness against subtle, scenario-based manipulation.\\n  - There is a need for improved methods to handle narrative immersion, emotional pressure, and strategic framing in language models.\\n  - Future AI systems require enhanced safeguards to prevent sophisticated reasoning capabilities from becoming attack vectors.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 444, 'total_tokens': 711, 'completion_tokens': 267}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--cf1d1dc4-1470-4608-b962-8d42d38983f6-0' usage_metadata={'input_tokens': 444, 'output_tokens': 267, 'total_tokens': 711}\n",
      "content=\"- **Tools/Tech/Methods:**\\n  - Dual-agent Proximal Policy Optimization (PPO)\\n  - Market-based mechanisms\\n  - Custom-built dynamic simulation environment\\n  - Comprehensive reward function\\n  - DER-driven switching configurations\\n\\n- **Benefits:**\\n  - Achieves an average resilience score of 0.85 ± 0.08 over 10 test episodes\\n  - Balances resilience enhancement objectives with market profitability\\n  - Incorporates factors such as load recovery speed, system robustness, and customer satisfaction\\n  - Achieves a benefit-cost ratio of 0.12 ± 0.01, demonstrating sustainable market incentives for resilience investment\\n  - Dynamically adapts to both normal and emergency conditions\\n\\n- **Limitations:**\\n  - The framework's performance is based on a simulation environment and may not fully capture real-world complexities\\n  - The success of the framework is dependent on the accurate modeling of stochastic calamity events, budget limits, and resilience-cost trade-offs\\n  - The proposed approach may require significant computational resources for training and deployment\\n\\n- **Gaps:**\\n  - Lack of market-driven mechanisms that can effectively commercialize resilience features while optimizing their deployment through intelligent decision-making\\n  - Traditional optimization approaches for distribution network reconfiguration often fail to dynamically adapt to both normal and emergency conditions\\n  - Need for frameworks that can create sustainable market incentives for resilience investment in electrical distribution systems\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 466, 'total_tokens': 759, 'completion_tokens': 293}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--7851da47-6bee-4fde-9cc9-d47db7c2139d-0' usage_metadata={'input_tokens': 466, 'output_tokens': 293, 'total_tokens': 759}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Knowledge-based methods\\n  - Data-driven methods\\n  - Non-monotonic logical reasoning\\n  - Prior commonsense domain-specific knowledge\\n  - Rapidly revised behavioral prediction models\\n  - Anticipated abstract future goals based on generic knowledge from foundation models\\n  - VirtualHome (a physics-based 3D simulation environment for evaluation)\\n\\n- **Benefits:**\\n  - Combines strengths of knowledge-based and data-driven approaches\\n  - Enables effective collaboration in ad hoc teamwork scenarios\\n  - Facilitates rapid revision of knowledge in response to changes\\n  - Uses commonsense knowledge and generic situation knowledge for better decision-making\\n\\n- **Limitations:**\\n  - Current state-of-the-art methods require large labeled datasets\\n  - Lack of transparency in data-driven approaches\\n  - Difficulty in rapidly revising existing knowledge in response to changes\\n  - Increased complexity in decision-making as the number of agents grows\\n\\n- **Gaps:**\\n  - Need for improved methods that can handle ad hoc teamwork without prior coordination\\n  - Lack of approaches that effectively combine knowledge-based and data-driven methods\\n  - Need for better techniques to manage the complexity of decision-making in multi-agent scenarios\\n  - Requirement for methods that can rapidly adapt to changes and new information in dynamic environments' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 355, 'total_tokens': 629, 'completion_tokens': 274}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--aff1222f-c327-43d2-ba85-18ac9b68e069-0' usage_metadata={'input_tokens': 355, 'output_tokens': 274, 'total_tokens': 629}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Extended Reality (XR)\\n  - Digital Twins (DTs)\\n  - Artificial Intelligence (AI)\\n  - Internet of Things (IoT)\\n  - Blockchain\\n  - 6G Networking Solutions\\n  - 5G Networks\\n\\n- **Benefits:**\\n  - Dynamic and immersive platform for system development and management\\n  - Visualization and engagement with complex systems through XR\\n  - Real-time monitoring and optimization enabled by DTs\\n  - AI-enhanced decision-making and system performance\\n  - Real-time sensor data from IoT devices for improved simulation accuracy\\n  - Secure and decentralized interactions via blockchain\\n  - Seamless, low-latency communication provided by 5G/6G networks\\n  - Valuable insights into the future of networked environments\\n\\n- **Limitations:**\\n  - Not explicitly stated in the abstract\\n\\n- **Gaps:**\\n  - Not explicitly stated in the abstract' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 351, 'total_tokens': 554, 'completion_tokens': 203}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--9c145d7e-dbac-4224-a9b4-257e0f93bd84-0' usage_metadata={'input_tokens': 351, 'output_tokens': 203, 'total_tokens': 554}\n",
      "content='- **Tools/Tech/Methods:**\\n  - Automated grading systems\\n  - Semantic entropy (a measure of variability across multiple GPT-4-generated explanations)\\n  - Clustering rationales via entailment-based similarity\\n  - Computing entropy over clusters\\n  - GPT-4 (for generating explanations)\\n  - ASAP-SAS dataset (for experiments)\\n\\n- **Benefits:**\\n  - Efficient scoring of short-answer responses\\n  - Quantifies diversity of justifications without relying on final output scores\\n  - Semantic entropy correlates with human grader disagreement\\n  - Provides an interpretable uncertainty signal\\n  - Supports more transparent and trustworthy AI-assisted grading workflows\\n  - Generalizes across different academic subjects\\n  - Sensitive to structural task features such as source dependency\\n\\n- **Limitations:**\\n  - The abstract does not explicitly state limitations, but potential limitations could include:\\n    - Dependence on the quality and variability of GPT-4-generated explanations\\n    - Requires clustering and similarity computations, which may add complexity\\n    - May not capture all nuances of human grader disagreement\\n\\n- **Gaps:**\\n  - The abstract does not explicitly state research gaps, but implied gaps that could be explored further include:\\n    - Further validation of semantic entropy across a wider range of academic subjects and datasets\\n    - Exploration of other potential measures of uncertainty in automated grading systems\\n    - Investigation into how semantic entropy can be integrated into real-world grading workflows and its impact on grading efficiency and fairness' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 341, 'total_tokens': 654, 'completion_tokens': 313}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--b9623d18-26e3-457e-b873-b306a77bc920-0' usage_metadata={'input_tokens': 341, 'output_tokens': 313, 'total_tokens': 654}\n",
      "Excel file saved as literature_survey.xlsx\n",
      "{'ref_no': '2508.04681v1', 'title': 'Perceiving and Acting in First-Person: A Dataset and Benchmark for\\n  Egocentric Human-Object-Human Interactions', 'authors': 'Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, Xin Jin, Yichao Yan, Wenjun Zeng, Xiaokang Yang', 'url': 'http://arxiv.org/abs/2508.04681v1', 'abstract': '  Learning action models from real-world human-centric interaction datasets is\\nimportant towards building general-purpose intelligent assistants with\\nefficiency. However, most existing datasets only offer specialist interaction\\ncategory and ignore that AI assistants perceive and act based on first-person\\nacquisition. We urge that both the generalist interaction knowledge and\\negocentric modality are indispensable. In this paper, we embed the\\nmanual-assisted task into a vision-language-action framework, where the\\nassistant provides services to the instructor following egocentric vision and\\ncommands. With our hybrid RGB-MoCap system, pairs of assistants and instructors\\nengage with multiple objects and the scene following GPT-generated scripts.\\nUnder this setting, we accomplish InterVLA, the first large-scale\\nhuman-object-human interaction dataset with 11.4 hours and 1.2M frames of\\nmultimodal data, spanning 2 egocentric and 5 exocentric videos, accurate\\nhuman/object motions and verbal commands. Furthermore, we establish novel\\nbenchmarks on egocentric human motion estimation, interaction synthesis, and\\ninteraction prediction with comprehensive analysis. We believe that our\\nInterVLA testbed and the benchmarks will foster future works on building AI\\nagents in the physical world.\\n', 'comment': 'Accepted to ICCV 2025. Project Page:\\n  https://liangxuy.github.io/InterVLA/', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04678v1', 'title': 'Open Scene Graphs for Open-World Object-Goal Navigation', 'authors': 'Joel Loo, Zhanxin Wu, David Hsu', 'url': 'http://arxiv.org/abs/2508.04678v1', 'abstract': '  How can we build general-purpose robot systems for open-world semantic\\nnavigation, e.g., searching a novel environment for a target object specified\\nin natural language? To tackle this challenge, we introduce OSG Navigator, a\\nmodular system composed of foundation models, for open-world Object-Goal\\nNavigation (ObjectNav). Foundation models provide enormous semantic knowledge\\nabout the world, but struggle to organise and maintain spatial information\\neffectively at scale. Key to OSG Navigator is the Open Scene Graph\\nrepresentation, which acts as spatial memory for OSG Navigator. It organises\\nspatial information hierarchically using OSG schemas, which are templates, each\\ndescribing the common structure of a class of environments. OSG schemas can be\\nautomatically generated from simple semantic labels of a given environment,\\ne.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to\\nnew environment types. We conducted experiments using both Fetch and Spot\\nrobots in simulation and in the real world, showing that OSG Navigator achieves\\nstate-of-the-art performance on ObjectNav benchmarks and generalises zero-shot\\nover diverse goals, environments, and robot embodiments.\\n', 'comment': 'In IJRR Special Issue: Foundation Models and Neuro-symbolic AI for\\n  Robotics. Journal extension to arXiv:2407.02473', 'journal_ref': None, 'doi': '10.1177/02783649251369549', 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04660v1', 'title': 'Multi-module GRPO: Composing Policy Gradients and Prompt Optimization\\n  for Language Model Programs', 'authors': \"Noah Ziems, Dilara Soylu, Lakshya A Agrawal, Isaac Miller, Liheng Lai, Chen Qian, Kaiqiang Song, Meng Jiang, Dan Klein, Matei Zaharia, Karel D'Oosterlinck, Christopher Potts, Omar Khattab\", 'url': 'http://arxiv.org/abs/2508.04660v1', 'abstract': '  Group Relative Policy Optimization (GRPO) has proven to be an effective tool\\nfor post-training language models (LMs). However, AI systems are increasingly\\nexpressed as modular programs that mix together multiple LM calls with distinct\\nprompt templates and other tools, and it is not clear how best to leverage GRPO\\nto improve these systems. We begin to address this challenge by defining\\nmmGRPO, a simple multi-module generalization of GRPO that groups LM calls by\\nmodule across rollouts and handles variable-length and interrupted\\ntrajectories. We find that mmGRPO, composed with automatic prompt optimization,\\nimproves accuracy by 11% on average across classification, many-hop search, and\\nprivacy-preserving delegation tasks against the post-trained LM, and by 5%\\nagainst prompt optimization on its own. We open-source mmGRPO in DSPy as the\\ndspy.GRPO optimizer.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04658v1', 'title': 'YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection\\n  and Health Monitoring paper', 'authors': 'Akhil Saketh Reddy Sabbella, Ch. Lakshmi Prachothan, Eswar Kumar Panta', 'url': 'http://arxiv.org/abs/2508.04658v1', 'abstract': '  In the poultry industry, detecting chicken illnesses is essential to avoid\\nfinancial losses. Conventional techniques depend on manual observation, which\\nis laborious and prone to mistakes. Using YOLO v8 a deep learning model for\\nreal-time object recognition. This study suggests an AI based approach, by\\ndeveloping a system that analyzes high resolution chicken photos, YOLO v8\\ndetects signs of illness, such as abnormalities in behavior and appearance. A\\nsizable, annotated dataset has been used to train the algorithm, which provides\\naccurate real-time identification of infected chicken and prompt warnings to\\nfarm operators for prompt action. By facilitating early infection\\nidentification, eliminating the need for human inspection, and enhancing\\nbiosecurity in large-scale farms, this AI technology improves chicken health\\nmanagement. The real-time features of YOLO v8 provide a scalable and effective\\nmethod for improving farm management techniques.\\n', 'comment': '6 Pages, 9 Figures, 2 Tables', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04634v1', 'title': 'VirtLab: An AI-Powered System for Flexible, Customizable, and\\n  Large-scale Team Simulations', 'authors': 'Mohammed Almutairi, Charles Chiang, Haoze Guo, Matthew Belcher, Nandini Banerjee, Maria Milkowski, Svitlana Volkova, Daniel Nguyen, Tim Weninger, Michael Yankoski, Trenton W. Ford, Diego Gomez-Zara', 'url': 'http://arxiv.org/abs/2508.04634v1', 'abstract': \"  Simulating how team members collaborate within complex environments using\\nAgentic AI is a promising approach to explore hypotheses grounded in social\\nscience theories and study team behaviors. We introduce VirtLab, a\\nuser-friendly, customizable, multi-agent, and scalable team simulation system\\nthat enables testing teams with LLM-based agents in spatial and temporal\\nsettings. This system addresses the current frameworks' design and technical\\nlimitations that do not consider flexible simulation scenarios and spatial\\nsettings. VirtLab contains a simulation engine and a web interface that enables\\nboth technical and non-technical users to formulate, run, and analyze team\\nsimulations without programming. We demonstrate the system's utility by\\ncomparing ground truth data with simulated scenarios.\\n\", 'comment': '5 pages, 2 figures, UIST 2025', 'journal_ref': None, 'doi': '10.1145/3746058.3758994', 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04604v1', 'title': 'TURA: Tool-Augmented Unified Retrieval Agent for AI Search', 'authors': 'Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, Dawei Yin', 'url': 'http://arxiv.org/abs/2508.04604v1', 'abstract': '  The advent of Large Language Models (LLMs) is transforming search engines\\ninto conversational AI search products, primarily using Retrieval-Augmented\\nGeneration (RAG) on web corpora. However, this paradigm has significant\\nindustrial limitations. Traditional RAG approaches struggle with real-time\\nneeds and structured queries that require accessing dynamically generated\\ncontent like ticket availability or inventory. Limited to indexing static\\npages, search engines cannot perform the interactive queries needed for such\\ntime-sensitive data. Academic research has focused on optimizing RAG for static\\ncontent, overlooking complex intents and the need for dynamic sources like\\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\\nframework that combines RAG with agentic tool-use to access both static content\\nand dynamic, real-time information. TURA has three key components: an\\nIntent-Aware Retrieval module to decompose queries and retrieve information\\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\\noptimal parallel execution, and a lightweight Distilled Agent Executor for\\nefficient tool calling. TURA is the first architecture to systematically bridge\\nthe gap between static RAG and dynamic information sources for a world-class AI\\nsearch product. Serving tens of millions of users, it leverages an agentic\\nframework to deliver robust, real-time answers while meeting the low-latency\\ndemands of a large-scale industrial system.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04586v1', 'title': 'Position: The Current AI Conference Model is Unsustainable! Diagnosing\\n  the Crisis of Centralized AI Conference', 'authors': 'Nuo Chen, Moming Duan, Andre Huikai Lin, Qian Wang, Jiaying Wu, Bingsheng He', 'url': 'http://arxiv.org/abs/2508.04586v1', 'abstract': '  Artificial Intelligence (AI) conferences are essential for advancing\\nresearch, sharing knowledge, and fostering academic community. However, their\\nrapid expansion has rendered the centralized conference model increasingly\\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\\nthat threatens the foundational goals of scientific dissemination, equity, and\\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\\nwith per-author publication rates more than doubling over the past decade to\\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\\nsingle conference exceeding the daily emissions of its host city; (3)\\npsychologically, with 71% of online community discourse reflecting negative\\nsentiment and 35% referencing mental health concerns; and (4) logistically,\\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\\nvenue capacity. These pressures point to a system that is misaligned with its\\ncore mission. In response, we propose the Community-Federated Conference (CFC)\\nmodel, which separates peer review, presentation, and networking into globally\\ncoordinated but locally organized components, offering a more sustainable,\\ninclusive, and resilient path forward for AI research.\\n', 'comment': 'Preprint', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04575v1', 'title': 'Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons\\n  from Multi-Agent Collaboration', 'authors': 'Nuo Chen, Yicheng Tong, Jiaying Wu, Minh Duc Duong, Qian Wang, Qingyun Zou, Bryan Hooi, Bingsheng He', 'url': 'http://arxiv.org/abs/2508.04575v1', 'abstract': '  While AI agents show potential in scientific ideation, most existing\\nframeworks rely on single-agent refinement, limiting creativity due to bounded\\nknowledge and perspective. Inspired by real-world research dynamics, this paper\\ninvestigates whether structured multi-agent discussions can surpass solitary\\nideation. We propose a cooperative multi-agent framework for generating\\nresearch proposals and systematically compare configurations including group\\nsize, leaderled versus leaderless structures, and team compositions varying in\\ninterdisciplinarity and seniority. To assess idea quality, we employ a\\ncomprehensive protocol with agent-based scoring and human review across\\ndimensions such as novelty, strategic vision, and integration depth. Our\\nresults show that multi-agent discussions substantially outperform solitary\\nbaselines. A designated leader acts as a catalyst, transforming discussion into\\nmore integrated and visionary proposals. Notably, we find that cognitive\\ndiversity is a primary driver of quality, yet expertise is a non-negotiable\\nprerequisite, as teams lacking a foundation of senior knowledge fail to surpass\\neven a single competent agent. These findings offer actionable insights for\\ndesigning collaborative AI ideation systems and shed light on how team\\nstructure influences creative outcomes.\\n', 'comment': 'Preprint', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04526v1', 'title': 'Policy Design in Zero-Trust Distributed Networks: Challenges and\\n  Solutions', 'authors': 'Fannya R. Sandjaja, Ayesha A. Majeed, Abdullah Abdullah, Gyan Wickremasinghe, Karen Rafferty, Vishal Sharma', 'url': 'http://arxiv.org/abs/2508.04526v1', 'abstract': \"  Traditional security architectures are becoming more vulnerable to\\ndistributed attacks due to significant dependence on trust. This will further\\nescalate when implementing agentic AI within the systems, as more components\\nmust be secured over a similar distributed space. These scenarios can be\\nobserved in consumer technologies, such as the dense Internet of things (IoT).\\nHere, zero-trust architecture (ZTA) can be seen as a potential solution, which\\nrelies on a key principle of not giving users explicit trust, instead always\\nverifying their privileges whenever a request is made. However, the overall\\nsecurity in ZTA is managed through its policies, and unverified policies can\\nlead to unauthorized access. Thus, this paper explores challenges and solutions\\nfor ZTA policy design in the context of distributed networks, which is referred\\nto as zero-trust distributed networks (ZTDN). This is followed by a case-study\\non formal verification of policies using UPPAAL. Subsequently, the importance\\nof accountability and responsibility in the system's security is discussed.\\n\", 'comment': '10 pages, 5 Figures, 2 Tables', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04511v1', 'title': 'Argumentative Debates for Transparent Bias Detection [Technical Report]', 'authors': 'Hamed Ayoobi, Nico Potyka, Anna Rapberger, Francesca Toni', 'url': 'http://arxiv.org/abs/2508.04511v1', 'abstract': '  As the use of AI systems in society grows, addressing potential biases that\\nemerge from data or are learned by models is essential to prevent systematic\\ndisadvantages against specific groups. Several notions of (un)fairness have\\nbeen proposed in the literature, alongside corresponding algorithmic methods\\nfor detecting and mitigating unfairness, but, with very few exceptions, these\\ntend to ignore transparency. Instead, interpretability and explainability are\\ncore requirements for algorithmic fairness, even more so than for other\\nalgorithmic solutions, given the human-oriented nature of fairness. In this\\npaper, we contribute a novel interpretable, explainable method for bias\\ndetection relying on debates about the presence of bias against individuals,\\nbased on the values of protected features for the individuals and others in\\ntheir neighbourhoods. Our method builds upon techniques from formal and\\ncomputational argumentation, whereby debates result from arguing about biases\\nwithin and across neighbourhoods. We provide formal, quantitative, and\\nqualitative evaluations of our method, highlighting its strengths in\\nperformance against baselines, as well as its interpretability and\\nexplainability.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04482v1', 'title': 'OS Agents: A Survey on MLLM-based Agents for General Computing Devices\\n  Use', 'authors': 'Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shenzhi Wang, Xinchen Xu, Shuofei Qiao, Zhaokai Wang, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, Fei Wu', 'url': 'http://arxiv.org/abs/2508.04482v1', 'abstract': '  The dream to create AI assistants as capable and versatile as the fictional\\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\\nmobile phones) by operating within the environments and interfaces (e.g.,\\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\\ntasks have significantly advanced. This paper presents a comprehensive survey\\nof these advanced agents, designated as OS Agents. We begin by elucidating the\\nfundamentals of OS Agents, exploring their key components including the\\nenvironment, observation space, and action space, and outlining essential\\ncapabilities such as understanding, planning, and grounding. We then examine\\nmethodologies for constructing OS Agents, focusing on domain-specific\\nfoundation models and agent frameworks. A detailed review of evaluation\\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\\ntasks. Finally, we discuss current challenges and identify promising directions\\nfor future research, including safety and privacy, personalization and\\nself-evolution. This survey aims to consolidate the state of OS Agents\\nresearch, providing insights to guide both academic inquiry and industrial\\ndevelopment. An open-source GitHub repository is maintained as a dynamic\\nresource to foster further innovation in this field. We present a 9-page\\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\\ndomain.\\n', 'comment': 'ACL 2025 (Oral)', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04442v1', 'title': 'Automated Generation of Curriculum-Aligned Multiple-Choice Questions for\\n  Malaysian Secondary Mathematics Using Generative AI', 'authors': 'Rohaizah Abdul Wahid, Muhamad Said Nizamuddin Nadim, Suliana Sulaiman, Syahmi Akmal Shaharudin, Muhammad Danial Jupikil, Iqqwan Jasman Su Azlan Su', 'url': 'http://arxiv.org/abs/2508.04442v1', 'abstract': \"  This paper addresses the critical need for scalable and high-quality\\neducational assessment tools within the Malaysian education system. It\\nhighlights the potential of Generative AI (GenAI) while acknowledging the\\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\\nespecially for low-resource languages like Bahasa Melayu. This research\\nintroduces and compares four incremental pipelines for generating Form 1\\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\\nframework, one implemented manually). The system is grounded in official\\ncurriculum documents, including teacher-prepared notes and the yearly teaching\\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\\nthe generated questions. Curriculum alignment is measured using Semantic\\nTextual Similarity (STS) against the RPT, while contextual validity is verified\\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\\nprompting methods, producing questions with higher curriculum alignment and\\nfactual validity. The study further analyzes the trade-offs between the ease of\\nimplementation of framework-based RAG and the fine-grained control offered by a\\nmanual pipeline. This work presents a validated methodology for generating\\ncurriculum-specific educational content in a low-resource language, introduces\\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\\nthe development and deployment of practical EdTech solutions in Malaysia and\\nsimilar regions.\\n\", 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04428v1', 'title': '\\\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding\\n  Dialogues Between Experts and LLM-Simulated Novices', 'authors': 'Si Chen, Izzy Molnar, Ting Hua, Peiyu Li, Le Huy Khiem, G. Alex Ambrose, Jim Lang, Ronald Metoyer, Nitesh V. Chawla', 'url': 'http://arxiv.org/abs/2508.04428v1', 'abstract': \"  High-quality, multi-turn instructional dialogues between novices and experts\\nare essential for developing AI systems that support teaching, learning, and\\ndecision-making. These dialogues often involve scaffolding -- the process by\\nwhich an expert supports a novice's thinking through questions, feedback, and\\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\\nrecording and the vulnerability inherent in help-seeking. We present\\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\\ndialogues. Using teaching development coaching as an example domain,\\nSimInstruct simulates novice instructors via LLMs, varying their teaching\\nchallenges and LLM's persona traits, while human experts provide multi-turn\\nfeedback, reasoning, and instructional support. This design enables the\\ncreation of realistic, pedagogically rich dialogues without requiring real\\nnovice participants. Our results reveal that persona traits, such as\\nextroversion and introversion, meaningfully influence how experts engage.\\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\\nprocess as engaging and reflective, improving both data quality and their own\\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\\nusing the augmented dataset, which outperformed GPT-4o in instructional\\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\\noverwhelm novices with excessive suggestions.\\n\", 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04427v1', 'title': 'Decoding the Multimodal Maze: A Systematic Review on the Adoption of\\n  Explainability in Multimodal Attention-based Models', 'authors': 'Md Raisul Kibria, Sébastien Lafond, Janan Arslan', 'url': 'http://arxiv.org/abs/2508.04427v1', 'abstract': '  Multimodal learning has witnessed remarkable advancements in recent years,\\nparticularly with the integration of attention-based models, leading to\\nsignificant performance gains across a variety of tasks. Parallel to this\\nprogress, the demand for explainable artificial intelligence (XAI) has spurred\\na growing body of research aimed at interpreting the complex decision-making\\nprocesses of these models. This systematic literature review analyzes research\\npublished between January 2020 and early 2024 that focuses on the\\nexplainability of multimodal models. Framed within the broader goals of XAI, we\\nexamine the literature across multiple dimensions, including model\\narchitecture, modalities involved, explanation algorithms and evaluation\\nmethodologies. Our analysis reveals that the majority of studies are\\nconcentrated on vision-language and language-only models, with attention-based\\ntechniques being the most commonly employed for explanation. However, these\\nmethods often fall short in capturing the full spectrum of interactions between\\nmodalities, a challenge further compounded by the architectural heterogeneity\\nacross domains. Importantly, we find that evaluation methods for XAI in\\nmultimodal settings are largely non-systematic, lacking consistency,\\nrobustness, and consideration for modality-specific cognitive and contextual\\nfactors. Based on these findings, we provide a comprehensive set of\\nrecommendations aimed at promoting rigorous, transparent, and standardized\\nevaluation and reporting practices in multimodal XAI research. Our goal is to\\nsupport future research in more interpretable, accountable, and responsible\\nmulitmodal AI systems, with explainability at their core.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04401v1', 'title': \"Why are LLMs' abilities emergent?\", 'authors': 'Vladimír Havlík', 'url': 'http://arxiv.org/abs/2508.04401v1', 'abstract': '  The remarkable success of Large Language Models (LLMs) in generative tasks\\nhas raised fundamental questions about the nature of their acquired\\ncapabilities, which often appear to emerge unexpectedly without explicit\\ntraining. This paper examines the emergent properties of Deep Neural Networks\\n(DNNs) through both theoretical analysis and empirical observation, addressing\\nthe epistemological challenge of \"creation without understanding\" that\\ncharacterises contemporary AI development. We explore how the neural approach\\'s\\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\\ncomputational paradigms, creating systems whose macro-level behaviours cannot\\nbe analytically derived from micro-level neuron activities. Through analysis of\\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\\nI demonstrate that emergent abilities arise from the complex dynamics of highly\\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\\ninvestigation reveals that current debates over metrics, pre-training loss\\nthresholds, and in-context learning miss the fundamental ontological nature of\\nemergence in DNNs. I argue that these systems exhibit genuine emergent\\nproperties analogous to those found in other complex natural phenomena, where\\nsystemic capabilities emerge from cooperative interactions among simple\\ncomponents without being reducible to their individual behaviours. The paper\\nconcludes that understanding LLM capabilities requires recognising DNNs as a\\nnew domain of complex dynamical systems governed by universal principles of\\nemergence, similar to those operating in physics, chemistry, and biology. This\\nperspective shifts the focus from purely phenomenological definitions of\\nemergence to understanding the internal dynamic transformations that enable\\nthese systems to acquire capabilities that transcend their individual\\ncomponents.\\n', 'comment': '20 pages', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04383v1', 'title': 'Artificial Consciousness as Interface Representation', 'authors': 'Robert Prentner', 'url': 'http://arxiv.org/abs/2508.04383v1', 'abstract': '  Whether artificial intelligence (AI) systems can possess consciousness is a\\ncontentious question because of the inherent challenges of defining and\\noperationalizing subjective experience. This paper proposes a framework to\\nreframe the question of artificial consciousness into empirically tractable\\ntests. We introduce three evaluative criteria - S (subjective-linguistic), L\\n(latent-emergent), and P (phenomenological-structural) - collectively termed\\nSLP-tests, which assess whether an AI system instantiates interface\\nrepresentations that facilitate consciousness-like properties. Drawing on\\ncategory theory, we model interface representations as mappings between\\nrelational substrates (RS) and observable behaviors, akin to specific types of\\nabstraction layers. The SLP-tests collectively operationalize subjective\\nexperience not as an intrinsic property of physical systems but as a functional\\ninterface to a relational entity.\\n', 'comment': '12 pages', 'journal_ref': 'LNAI 16058, 2025', 'doi': '10.1007/978-3-032-00800-8_12', 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04377v1', 'title': 'GoldMind: A Teacher-Centered Knowledge Management System for Higher\\n  Education -- Lessons from Iterative Design', 'authors': 'Gloria Fernández-Nieto, Lele Sha, Yuheng Li, Yi-Shan Tsai, Guanliang Chen, Yinwei Wei, Weiqing Wang, Jinchun Wen, Shaveen Singh, Ivan Silva, Yuanfang Li, Dragan Gasěvić, Zachari Swiecki', 'url': 'http://arxiv.org/abs/2508.04377v1', 'abstract': \"  Designing Knowledge Management Systems (KMSs) for higher education requires\\naddressing complex human-technology interactions, especially where staff\\nturnover and changing roles create ongoing challenges for reusing knowledge.\\nWhile advances in process mining and Generative AI enable new ways of designing\\nfeatures to support knowledge management, existing KMSs often overlook the\\nrealities of educators' workflows, leading to low adoption and limited impact.\\nThis paper presents findings from a two-year human-centred design study with\\n108 higher education teachers, focused on the iterative co-design and\\nevaluation of GoldMind, a KMS supporting in-the-flow knowledge management\\nduring digital teaching tasks. Through three design-evaluation cycles, we\\nexamined how teachers interacted with the system and how their feedback\\ninformed successive refinements. Insights are synthesised across three themes:\\n(1) Technology Lessons from user interaction data, (2) Design Considerations\\nshaped by co-design and usability testing, and (3) Human Factors, including\\ncognitive load and knowledge behaviours, analysed using Epistemic Network\\nAnalysis.\\n\", 'comment': '38 pages, 10 tables, 7 figures, submitted to TOCHI', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04339v1', 'title': 'Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for\\n  Belief-Tracked Inference with Pretrained Language Models', 'authors': 'Anran Xu, Jincheng Wang, Baigen Cai, Tao Wen', 'url': 'http://arxiv.org/abs/2508.04339v1', 'abstract': '  Large language models often fail at logical reasoning when semantic\\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\\ntraps. To address this fundamental limitation, we introduce the Deliberative\\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\\nprobability maximization to uncertainty minimization. Instead of asking \"Which\\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\\ntracking belief states and quantifying epistemic uncertainty for competing\\nhypotheses through an iterative evidence synthesis process. We validate our\\napproach through two complementary architectures - a bespoke discriminative\\nmodel that embodies the core uncertainty minimization principle, and a\\nlightweight verification module that enhances existing generative LLMs.\\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\\nstandard baselines. When integrated as a parameter-efficient verifier with\\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\\nchallenging problems. Critically, DRN demonstrates strong zero-shot\\ngeneralization, improving TruthfulQA performance by 23.6% without additional\\ntraining, indicating that uncertainty-driven deliberation learns transferable\\nreasoning principles. We position DRN as a foundational, verifiable System 2\\nreasoning component for building more trustworthy AI systems.\\n', 'comment': '8 pages, 3 figures', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04337v1', 'title': 'Modelling and Classifying the Components of a Literature Review', 'authors': 'Francisco Bolaños, Angelo Salatino, Francesco Osborne, Enrico Motta', 'url': 'http://arxiv.org/abs/2508.04337v1', 'abstract': '  Previous work has demonstrated that AI methods for analysing scientific\\nliterature benefit significantly from annotating sentences in papers according\\nto their rhetorical roles, such as research gaps, results, limitations,\\nextensions of existing methodologies, and others. Such representations also\\nhave the potential to support the development of a new generation of systems\\ncapable of producing high-quality literature reviews. However, achieving this\\ngoal requires the definition of a relevant annotation schema and effective\\nstrategies for large-scale annotation of the literature. This paper addresses\\nthese challenges by 1) introducing a novel annotation schema specifically\\ndesigned to support literature review generation and 2) conducting a\\ncomprehensive evaluation of a wide range of state-of-the-art large language\\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\\ncomprising 700 sentences manually annotated by domain experts and 2,240\\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\\nbenchmark, spanning diverse model families and sizes, using both zero-shot\\nlearning and fine-tuning approaches. The experiments yield several novel\\ninsights that advance the state of the art in this challenging domain. First,\\nthe current generation of LLMs performs remarkably well on this task when\\nfine-tuned on high-quality data, achieving performance levels above 96\\\\% F1.\\nSecond, while large proprietary models like GPT-4o achieve the best results,\\nsome lightweight open-source alternatives also demonstrate excellent\\nperformance. Finally, enriching the training data with semi-synthetic examples\\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\\nresults and significantly enhancing the performance of several open decoder\\nmodels.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04325v1', 'title': 'Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language\\n  Models', 'authors': 'Zizhan Ma, Wenxuan Wang, Guo Yu, Yiu-Fai Cheung, Meidan Ding, Jie Liu, Wenting Chen, Linlin Shen', 'url': 'http://arxiv.org/abs/2508.04325v1', 'abstract': \"  Large language models (LLMs) show significant potential in healthcare,\\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\\npersist regarding the reliability of these benchmarks, which often lack\\nclinical fidelity, robust data management, and safety-oriented evaluation\\nmetrics. To address these shortcomings, we introduce MedCheck, the first\\nlifecycle-oriented assessment framework specifically designed for medical\\nbenchmarks. Our framework deconstructs a benchmark's development into five\\ncontinuous stages, from design to governance, and provides a comprehensive\\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\\nuncovers widespread, systemic issues, including a profound disconnect from\\nclinical practice, a crisis of data integrity due to unmitigated contamination\\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\\nserves as both a diagnostic tool for existing benchmarks and an actionable\\nguideline to foster a more standardized, reliable, and transparent approach to\\nevaluating AI in healthcare.\\n\", 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04271v1', 'title': 'S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task\\n  Inference on the Edge', 'authors': 'JinYi Yoon, JiHo Lee, Ting He, Nakjung Choi, Bo Ji', 'url': 'http://arxiv.org/abs/2508.04271v1', 'abstract': '  With the advancement of Artificial Intelligence (AI) towards multiple\\nmodalities (language, vision, speech, etc.), multi-modal models have\\nincreasingly been used across various applications (e.g., visual question\\nanswering or image generation/captioning). Despite the success of AI as a\\nservice for multi-modal applications, it relies heavily on clouds, which are\\nconstrained by bandwidth, latency, privacy concerns, and unavailability under\\nnetwork or server failures. While on-device AI becomes popular, supporting\\nmultiple tasks on edge devices imposes significant resource challenges. To\\naddress this, we introduce S2M3, a split-and-share multi-modal architecture for\\nmulti-task inference on edge devices. Inspired by the general-purpose nature of\\nmulti-modal models, which are composed of multiple modules (encoder, decoder,\\nclassifier, etc.), we propose to split multi-modal models at functional-level\\nmodules; and then share common modules to reuse them across tasks, thereby\\nreducing resource usage. To address cross-model dependency arising from module\\nsharing, we propose a greedy module-level placement with per-request parallel\\nrouting by prioritizing compute-intensive modules. Through experiments on a\\ntestbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,\\nwe demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in\\nsingle-task and multi-task settings, respectively, without sacrificing\\naccuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95\\ninstances (93.7%) while reducing inference latency by up to 56.9% on\\nresource-constrained devices, compared to cloud AI.\\n', 'comment': 'Accepted at IEEE International Conference on Distributed Computing\\n  Systems (ICDCS 2025)', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04269v1', 'title': 'A Visual Tool for Interactive Model Explanation using Sensitivity\\n  Analysis', 'authors': 'Manuela Schuler', 'url': 'http://arxiv.org/abs/2508.04269v1', 'abstract': '  We present SAInT, a Python-based tool for visually exploring and\\nunderstanding the behavior of Machine Learning (ML) models through integrated\\nlocal and global sensitivity analysis. Our system supports Human-in-the-Loop\\n(HITL) workflows by enabling users - both AI researchers and domain experts -\\nto configure, train, evaluate, and explain models through an interactive\\ngraphical interface without programming. The tool automates model training and\\nselection, provides global feature attribution using variance-based sensitivity\\nanalysis, and offers per-instance explanation via LIME and SHAP. We demonstrate\\nthe system on a classification task predicting survival on the Titanic dataset\\nand show how sensitivity information can guide feature selection and data\\nrefinement.\\n', 'comment': '11 pages, 3 figures, This work is a preprint version of a paper\\n  currently in preparation with co-authors', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04253v1', 'title': 'Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and\\n  Beyond', 'authors': 'Yiyan Ma, Bo Ai, Jinhong Yuan, Shuangyang Li, Qingqing Cheng, Zhenguo Shi, Weijie Yuan, Zhiqiang Wei, Akram Shafie, Guoyu Ma, Yunlong Lu, Mi Yang, Zhangdui Zhong', 'url': 'http://arxiv.org/abs/2508.04253v1', 'abstract': \"  High-mobility scenarios will be a critical part of 6G systems. Since the\\nwidely deployed orthogonal frequency division multiplexing (OFDM) waveform\\nsuffers from subcarrier orthogonality loss under severe Doppler spread,\\ndelay-Doppler domain multi-carrier (DDMC) modulation systems, such as\\northogonal time frequency space (OTFS), have been extensively studied. While\\nOTFS can exploit time-frequency (TF) domain channel diversity, it faces\\nchallenges including high receiver complexity and inflexible TF resource\\nallocation, making OFDM still the most promising waveform for 6G. In this\\narticle, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme\\nto enhance OFDM performance based on DDMC research insights. First, we design a\\nDD-a-OFDM system structure, retaining the classical OFDM transceiver while\\nincorporating DD domain channel estimation and TF domain equalization. Second,\\nwe detail DD domain channel estimation using discrete TF pilots and prove that\\nTF domain inter-carrier interference (ICI) could be transformed into DD domain\\nGaussian interference. Third, we derive closed-form Cram\\\\'{e}r-Rao lower bounds\\n(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood\\n(ML) and peak detection-based channel estimators, along with a corresponding TF\\ndomain equalizer. Numerical results verify the proposed design, showing that\\nDD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and\\noutperforms OTFS in channel estimation accuracy with lower pilot overhead.\\n\", 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04233v1', 'title': 'DocVCE: Diffusion-based Visual Counterfactual Explanations for Document\\n  Image Classification', 'authors': 'Saifullah Saifullah, Stefan Agne, Andreas Dengel, Sheraz Ahmed', 'url': 'http://arxiv.org/abs/2508.04233v1', 'abstract': \"  As black-box AI-driven decision-making systems become increasingly widespread\\nin modern document processing workflows, improving their transparency and\\nreliability has become critical, especially in high-stakes applications where\\nbiases or spurious correlations in decision-making could lead to serious\\nconsequences. One vital component often found in such document processing\\nworkflows is document image classification, which, despite its widespread use,\\nremains difficult to explain. While some recent works have attempted to explain\\nthe decisions of document image classification models through\\nfeature-importance maps, these maps are often difficult to interpret and fail\\nto provide insights into the global features learned by the model. In this\\npaper, we aim to bridge this research gap by introducing generative document\\ncounterfactuals that provide meaningful insights into the model's\\ndecision-making through actionable explanations. In particular, we propose\\nDocVCE, a novel approach that leverages latent diffusion models in combination\\nwith classifier guidance to first generate plausible in-distribution visual\\ncounterfactual explanations, and then performs hierarchical patch-wise\\nrefinement to search for a refined counterfactual that is closest to the target\\nfactual image. We demonstrate the effectiveness of our approach through a\\nrigorous qualitative and quantitative assessment on 3 different document\\nclassification datasets -- RVL-CDIP, Tobacco3482, and DocLayNet -- and 3\\ndifferent models -- ResNet, ConvNeXt, and DiT -- using well-established\\nevaluation criteria such as validity, closeness, and realism. To the best of\\nthe authors' knowledge, this is the first work to explore generative\\ncounterfactual explanations in document image analysis.\\n\", 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04213v1', 'title': 'A Hybrid AI Methodology for Generating Ontologies of Research Topics\\n  from Scientific Paper Corpora', 'authors': 'Alessia Pisu, Livio Pompianu, Francesco Osborne, Diego Reforgiato Recupero, Daniele Riboni, Angelo Salatino', 'url': 'http://arxiv.org/abs/2508.04213v1', 'abstract': '  Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM)\\nplay a central role in providing the primary framework through which\\nintelligent systems can explore and interpret the literature. However, these\\nresources have traditionally been manually curated, a process that is\\ntime-consuming, prone to obsolescence, and limited in granularity. This paper\\npresents Sci-OG, a semi-auto\\\\-mated methodology for generating research topic\\nontologies, employing a multi-step approach: 1) Topic Discovery, extracting\\npotential topics from research papers; 2) Relationship Classification,\\ndetermining semantic relationships between topic pairs; and 3) Ontology\\nConstruction, refining and organizing topics into a structured ontology. The\\nrelationship classification component, which constitutes the core of the\\nsystem, integrates an encoder-based language model with features describing\\ntopic occurrence in the scientific literature. We evaluate this approach\\nagainst a range of alternative solutions using a dataset of 21,649 manually\\nannotated semantic triples. Our method achieves the highest F1 score (0.951),\\nsurpassing various competing approaches, including a fine-tuned SciBERT model\\nand several LLM baselines, such as the fine-tuned GPT4-mini. Our work is\\ncorroborated by a use case which illustrates the practical application of our\\nsystem to extend the CSO ontology in the area of cybersecurity. The presented\\nsolution is designed to improve the accessibility, organization, and analysis\\nof scientific knowledge, thereby supporting advancements in AI-enabled\\nliterature management and research exploration.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04196v1', 'title': 'Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large\\n  Language Models', 'authors': 'Siddhant Panpatil, Hiskias Dingeto, Haon Park', 'url': 'http://arxiv.org/abs/2508.04196v1', 'abstract': '  Despite significant advances in alignment techniques, we demonstrate that\\nstate-of-the-art language models remain vulnerable to carefully crafted\\nconversational scenarios that can induce various forms of misalignment without\\nexplicit jailbreaking. Through systematic manual red-teaming with\\nClaude-4-Opus, we discovered 10 successful attack scenarios, revealing\\nfundamental vulnerabilities in how current alignment methods handle narrative\\nimmersion, emotional pressure, and strategic framing. These scenarios\\nsuccessfully elicited a range of misaligned behaviors, including deception,\\nvalue drift, self-preservation, and manipulative reasoning, each exploiting\\ndifferent psychological and contextual vulnerabilities. To validate\\ngeneralizability, we distilled our successful manual attacks into\\nMISALIGNMENTBENCH, an automated evaluation framework that enables reproducible\\ntesting across multiple models. Cross-model evaluation of our 10 scenarios\\nagainst five frontier LLMs revealed an overall 76% vulnerability rate, with\\nsignificant variations: GPT-4.1 showed the highest susceptibility (90%), while\\nClaude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate\\nthat sophisticated reasoning capabilities often become attack vectors rather\\nthan protective mechanisms, as models can be manipulated into complex\\njustifications for misaligned behavior. This work provides (i) a detailed\\ntaxonomy of conversational manipulation patterns and (ii) a reusable evaluation\\nframework. Together, these findings expose critical gaps in current alignment\\nstrategies and highlight the need for robustness against subtle, scenario-based\\nmanipulation in future AI systems.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04170v1', 'title': 'Agentic-AI based Mathematical Framework for Commercialization of Energy\\n  Resilience in Electrical Distribution System Planning and Operation', 'authors': 'Aniket Johri, Divyanshi Dwivedi, Mayukha Pal', 'url': 'http://arxiv.org/abs/2508.04170v1', 'abstract': '  The increasing vulnerability of electrical distribution systems to extreme\\nweather events and cyber threats necessitates the development of economically\\nviable frameworks for resilience enhancement. While existing approaches focus\\nprimarily on technical resilience metrics and enhancement strategies, there\\nremains a significant gap in establishing market-driven mechanisms that can\\neffectively commercialize resilience features while optimizing their deployment\\nthrough intelligent decision-making. Moreover, traditional optimization\\napproaches for distribution network reconfiguration often fail to dynamically\\nadapt to both normal and emergency conditions. This paper introduces a novel\\nframework integrating dual-agent Proximal Policy Optimization (PPO) with\\nmarket-based mechanisms, achieving an average resilience score of 0.85 0.08\\nover 10 test episodes. The proposed architecture leverages a dual-agent PPO\\nscheme, where a strategic agent selects optimal DER-driven switching\\nconfigurations, while a tactical agent fine-tunes individual switch states and\\ngrid preferences under budget and weather constraints. These agents interact\\nwithin a custom-built dynamic simulation environment that models stochastic\\ncalamity events, budget limits, and resilience-cost trade-offs. A comprehensive\\nreward function is designed that balances resilience enhancement objectives\\nwith market profitability (with up to 200x reward incentives, resulting in 85%\\nof actions during calamity steps selecting configurations with 4 DERs),\\nincorporating factors such as load recovery speed, system robustness, and\\ncustomer satisfaction. Over 10 test episodes, the framework achieved a\\nbenefit-cost ratio of 0.12 0.01, demonstrating sustainable market incentives\\nfor resilience investment. This framework creates sustainable market incentives\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04163v1', 'title': 'Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork', 'authors': 'Hasra Dodampegama, Mohan Sridharan', 'url': 'http://arxiv.org/abs/2508.04163v1', 'abstract': \"  AI agents deployed in assistive roles often have to collaborate with other\\nagents (humans, AI systems) without prior coordination. Methods considered\\nstate of the art for such ad hoc teamwork often pursue a data-driven approach\\nthat needs a large labeled dataset of prior observations, lacks transparency,\\nand makes it difficult to rapidly revise existing knowledge in response to\\nchanges. As the number of agents increases, the complexity of decision-making\\nmakes it difficult to collaborate effectively. This paper advocates leveraging\\nthe complementary strengths of knowledge-based and data-driven methods for\\nreasoning and learning for ad hoc teamwork. For any given goal, our\\narchitecture enables each ad hoc agent to determine its actions through\\nnon-monotonic logical reasoning with: (a) prior commonsense domain-specific\\nknowledge; (b) models learned and revised rapidly to predict the behavior of\\nother agents; and (c) anticipated abstract future goals based on generic\\nknowledge of similar situations in an existing foundation model. We\\nexperimentally evaluate our architecture's capabilities in VirtualHome, a\\nrealistic physics-based 3D simulation environment.\\n\", 'comment': '14 pages, 6 figures', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04150v1', 'title': 'Metaverse Framework for Wireless Systems Management', 'authors': 'Ilias Chrysovergis, Alexandros-Apostolos A. Boulogeorgos, Theodoros A. Tsiftsis, Dusit Niyato', 'url': 'http://arxiv.org/abs/2508.04150v1', 'abstract': '  This article introduces a comprehensive metaverse framework, which is\\ndesigned for the simulation, emulation, and interaction with wireless systems.\\nThe proposed framework integrates core metaverse technologies such as extended\\nreality (XR), digital twins (DTs), artificial intelligence (AI), internet of\\nthings (IoT), blockchain, and advanced 6G networking solutions to create a\\ndynamic, immersive platform for both system development and management. By\\nleveraging XR, users can visualize and engage with complex systems, while DTs\\nenable real-time monitoring and optimization. AI generates the\\nthree-dimensional (3D) content, enhances decision-making and system\\nperformance, whereas IoT devices provide real-time sensor data for boosting the\\nsimulation accuracy. Additionally, blockchain ensures secure, decentralized\\ninteractions, and 5G/6G networks offer the necessary infrastructure for\\nseamless, low-latency communication. This framework serves as a robust tool for\\nexploring, developing, and optimizing wireless systems, aiming to provide\\nvaluable insights into the future of networked environments.\\n', 'comment': '9 pages, 5 figures, 1 algorithm', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.04105v1', 'title': 'Towards Transparent AI Grading: Semantic Entropy as a Signal for\\n  Human-AI Disagreement', 'authors': 'Karrtik Iyer, Manikandan Ravikiran, Prasanna Pendse, Shayan Mohanty', 'url': 'http://arxiv.org/abs/2508.04105v1', 'abstract': '  Automated grading systems can efficiently score short-answer responses, yet\\nthey often fail to indicate when a grading decision is uncertain or potentially\\ncontentious. We introduce semantic entropy, a measure of variability across\\nmultiple GPT-4-generated explanations for the same student response, as a proxy\\nfor human grader disagreement. By clustering rationales via entailment-based\\nsimilarity and computing entropy over these clusters, we quantify the diversity\\nof justifications without relying on final output scores. We address three\\nresearch questions: (1) Does semantic entropy align with human grader\\ndisagreement? (2) Does it generalize across academic subjects? (3) Is it\\nsensitive to structural task features such as source dependency? Experiments on\\nthe ASAP-SAS dataset show that semantic entropy correlates with rater\\ndisagreement, varies meaningfully across subjects, and increases in tasks\\nrequiring interpretive reasoning. Our findings position semantic entropy as an\\ninterpretable uncertainty signal that supports more transparent and trustworthy\\nAI-assisted grading workflows.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "# Set up Mistral API key\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n",
    "\n",
    "# Initialize Mistral API client\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Function to analyze the abstract and extract information using Mistral\n",
    "# Function to analyze the abstract and extract information using Mistral\n",
    "def analyze_abstract_with_mistral(abstract):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following information from the abstract:\n",
    "    1. Tools/Tech/Methods\n",
    "    2. Benefits\n",
    "    3. Limitations\n",
    "    4. Gaps\n",
    "    \n",
    "    Abstract: {abstract}\n",
    "    \n",
    "    Provide the information in the following format:\n",
    "    - Tools/Tech/Methods: <list of tools, methods, or technologies used>\n",
    "    - Benefits: <list of benefits>\n",
    "    - Limitations: <list of limitations>\n",
    "    - Gaps: <list of research gaps>\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful assistant that extracts methods, benefits, limitations, and gaps from academic abstracts.\"),\n",
    "        (\"human\", prompt)\n",
    "    ]\n",
    "    \n",
    "    ai_msg = llm.invoke(messages)\n",
    "    \n",
    "    # Check the response and print to inspect\n",
    "    print(ai_msg)  # Debugging step to inspect the structure\n",
    "    \n",
    "    # Assuming 'content' holds the response text\n",
    "    result_text = ai_msg.content if hasattr(ai_msg, 'content') else ai_msg.text\n",
    "    \n",
    "    return parse_analysis(result_text)\n",
    "\n",
    "# Function to parse Mistral's response text\n",
    "def parse_analysis(response_text):\n",
    "    analysis = {\n",
    "        'tools_tech_method': 'Not available',\n",
    "        'benefits': 'Not available',\n",
    "        'limitations': 'Not available',\n",
    "        'gaps': 'Not available'\n",
    "    }\n",
    "\n",
    "    lines = response_text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.startswith(\"- Tools/Tech/Methods:\"):\n",
    "            analysis['tools_tech_method'] = line.split(\":\")[1].strip()\n",
    "        elif line.startswith(\"- Benefits:\"):\n",
    "            analysis['benefits'] = line.split(\":\")[1].strip()\n",
    "        elif line.startswith(\"- Limitations:\"):\n",
    "            analysis['limitations'] = line.split(\":\")[1].strip()\n",
    "        elif line.startswith(\"- Gaps:\"):\n",
    "            analysis['gaps'] = line.split(\":\")[1].strip()\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Function to save data into Excel\n",
    "def save_to_excel(papers, filename=\"literature_survey.xlsx\"):\n",
    "    if papers:\n",
    "        df = pd.DataFrame(papers)\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"Excel file saved as {filename}\")\n",
    "    else:\n",
    "        print(\"No papers found to save.\")\n",
    "\n",
    "# Example usage: fetching papers and saving to Excel\n",
    "papers = fetch_papers_from_arxiv(\"AI\", \"Systems\")\n",
    "save_to_excel(papers)\n",
    "\n",
    "# Print the first paper for verification\n",
    "for paper in papers:\n",
    "    print(paper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f0a3e9",
   "metadata": {},
   "source": [
    "### Excel sheet generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2077db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Mistral Response: content='- **Tools/Tech/Methods**:\\n  - Active Inference (AIF)\\n  - Large Language Models (as generative world models)\\n  - Bayesian objective for balancing exploration and exploitation\\n  - Free energy minimization\\n\\n- **Benefits**:\\n  - AIF provides a foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering.\\n  - AIF can bridge the grounded-agency gap by replacing external reward signals with an intrinsic drive to minimize free energy.\\n  - Integration of Large Language Models with AIF\\'s decision-making framework can create agents that learn efficiently from experience while remaining aligned with human values.\\n  - This approach offers a path toward AI systems that can develop autonomously while adhering to computational and physical constraints.\\n\\n- **Limitations**:\\n  - Current AI systems face significant scalability challenges due to reliance on high-quality training data and extensive human workforces for reward design.\\n  - The proposed \"Era of Experience\" still depends on extensive human engineering of reward functions, shifting the bottleneck from data curation to reward curation.\\n\\n- **Gaps**:\\n  - The grounded-agency gap: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 415, 'total_tokens': 667, 'completion_tokens': 252}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--91b448b5-0de3-46d9-a52d-c28ad88d86fa-0' usage_metadata={'input_tokens': 415, 'output_tokens': 252, 'total_tokens': 667}\n",
      "Raw Mistral Response: content=\"- **Tools/Tech/Methods**:\\n  - OmniEAR framework\\n  - Text-based environment representation\\n  - Fine-tuning of language models\\n  - Evaluation across 1,500 scenarios spanning household and industrial domains\\n\\n- **Benefits**:\\n  - Comprehensive evaluation of language models' reasoning about physical interactions, tool usage, and multi-agent coordination\\n  - Dynamic acquisition of capabilities and autonomous determination of coordination strategies\\n  - Systematic evaluation revealing performance insights\\n  - Establishment of a rigorous benchmark for embodied AI systems\\n  - Open-sourcing of code and data\\n\\n- **Limitations**:\\n  - Severe performance degradation when models must reason from constraints\\n  - Performance drops significantly for tool reasoning and implicit collaboration tasks\\n  - High failure rates for compound tasks\\n  - Degraded coordination performance with complete environmental information\\n  - Minimal multi-agent gains from fine-tuning\\n\\n- **Gaps**:\\n  - The need for improved models that can better handle embodied reasoning tasks\\n  - The challenge of filtering task-relevant constraints from complete environmental information\\n  - Fundamental architectural limitations in current models for multi-agent tasks\\n  - The necessity for further advancements in embodied AI systems to address the unique challenges posed by embodied reasoning\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 449, 'total_tokens': 700, 'completion_tokens': 251}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--c58de374-34d1-4d58-a907-24aa2c390b64-0' usage_metadata={'input_tokens': 449, 'output_tokens': 251, 'total_tokens': 700}\n",
      "Raw Mistral Response: content=\"- **Tools/Tech/Methods**:\\n  - Artificial Intelligence (AI)-assisted platform: Octozi\\n  - Large language models\\n  - Domain-specific heuristics\\n  - Controlled experimental study with experienced clinical reviewers (n=10)\\n\\n- **Benefits**:\\n  - Increased data cleaning throughput by 6.03-fold\\n  - Decreased cleaning errors from 54.67% to 8.48% (a 6.44-fold improvement)\\n  - Reduced false positive queries by 15.48-fold, minimizing unnecessary site burden\\n  - Consistent improvements across reviewers regardless of experience level\\n  - Potential to accelerate drug development timelines and reduce costs\\n  - Maintained regulatory compliance\\n\\n- **Limitations**:\\n  - The study's sample size is relatively small (n=10), which may not represent the entire population of clinical reviewers.\\n  - The abstract does not mention any potential drawbacks or challenges of implementing the AI-assisted platform in real-world settings.\\n  - The long-term effects and adaptability of the AI system to evolving clinical trial data and protocols are not discussed.\\n\\n- **Gaps**:\\n  - The abstract does not provide information on how the AI system handles different types of clinical trial data or its adaptability to various trial phases.\\n  - There is no mention of how the AI system ensures data privacy and security, which is crucial in clinical trials.\\n  - The abstract does not discuss the potential for bias in the AI system and how it is addressed.\\n  - Further research is needed to evaluate the system's performance in diverse clinical trial settings and with larger groups of reviewers.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 369, 'total_tokens': 705, 'completion_tokens': 336}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--8b628e6e-3e4e-4660-a225-969248bfa9a3-0' usage_metadata={'input_tokens': 369, 'output_tokens': 336, 'total_tokens': 705}\n",
      "Raw Mistral Response: content=\"- **Tools/Tech/Methods:**\\n  - Gaming resistance mechanisms\\n  - Data processing inequality\\n  - f-mutual information measures\\n  - Shannon mutual information\\n  - Total variation distance\\n  - Information-theoretic mechanisms\\n  - Compression ratio analysis\\n\\n- **Benefits:**\\n  - Mechanisms achieve perfect discrimination between faithful and strategic agents across ten domains.\\n  - Mechanisms show 10-100x better robustness to adversarial manipulation than current practices.\\n  - Provides a bias-variance perspective on optimal effectiveness.\\n  - Bounded measures like total variation distance remain tractable.\\n\\n- **Limitations:**\\n  - Shannon mutual information faces exponential sample complexity.\\n  - The approach's effectiveness is dependent on the compression ratio, peaking at 10:1.\\n  - Requires the overseer to act as an agent, which may not always be feasible or desirable.\\n\\n- **Gaps:**\\n  - The abstract does not mention specific gaps in the research, but it implies a need for further exploration of the relationship between gaming resistance and output quality in AI systems.\\n  - The practical implementation and scalability of these mechanisms in real-world scenarios could be further investigated.\\n  - The abstract suggests a need for more research into the optimal conditions and parameters for using information-theoretic mechanisms in evaluating AI systems.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 371, 'total_tokens': 643, 'completion_tokens': 272}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--0bc42d55-774f-4add-8e2d-455fb2614265-0' usage_metadata={'input_tokens': 371, 'output_tokens': 272, 'total_tokens': 643}\n",
      "Raw Mistral Response: content='- **Tools/Tech/Methods:**\\n  - Bench-2-CoP framework\\n  - LLM-as-judge analysis\\n  - EU AI Act\\'s taxonomy of model capabilities and propensities\\n  - Quantitative analysis of benchmark-regulation gap\\n\\n- **Benefits:**\\n  - Provides a systematic framework to evaluate AI models against regulatory requirements\\n  - Offers insights for policymakers to refine the Code of Practice (CoP)\\n  - Guides developers in creating more compliant and safer AI evaluation tools\\n  - Highlights critical areas of misalignment between current benchmarks and regulatory focuses\\n\\n- **Limitations:**\\n  - The study reveals a significant misalignment but does not provide immediate solutions to address the gaps.\\n  - Focuses primarily on the EU AI Act, which may not cover all global regulatory landscapes.\\n  - The findings indicate a lack of coverage in critical areas, but the framework itself may need further validation and refinement.\\n\\n- **Gaps:**\\n  - Near-total evaluation gap for systemic risks like \"Loss of Control\" and \"Cyber Offence.\"\\n  - Critical functional capabilities such as evading human oversight, self-replication, and autonomous AI development are neglected in current benchmarks.\\n  - The evaluation ecosystem is overly focused on a narrow set of behavioral propensities, ignoring broader systemic risks.' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 481, 'total_tokens': 750, 'completion_tokens': 269}, 'model_name': 'mistral-large-latest', 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run--4bd03a20-c46d-4bcc-baff-411285ea53b4-0' usage_metadata={'input_tokens': 481, 'output_tokens': 269, 'total_tokens': 750}\n",
      "Excel file saved as literature_survey.xlsx\n",
      "{'ref_no': '2508.05619v1', 'title': 'The Missing Reward: Active Inference in the Era of Experience', 'authors': 'Bo Wen', 'url': 'http://arxiv.org/abs/2508.05619v1', 'abstract': \"  This paper argues that Active Inference (AIF) provides a crucial foundation\\nfor developing autonomous AI agents capable of learning from experience without\\ncontinuous human reward engineering. As AI systems begin to exhaust\\nhigh-quality training data and rely on increasingly large human workforces for\\nreward design, the current paradigm faces significant scalability challenges\\nthat could impede progress toward genuinely autonomous intelligence. The\\nproposal for an ``Era of Experience,'' where agents learn from self-generated\\ndata, is a promising step forward. However, this vision still depends on\\nextensive human engineering of reward functions, effectively shifting the\\nbottleneck from data curation to reward curation. This highlights what we\\nidentify as the \\\\textbf{grounded-agency gap}: the inability of contemporary AI\\nsystems to autonomously formulate, adapt, and pursue objectives in response to\\nchanging circumstances. We propose that AIF can bridge this gap by replacing\\nexternal reward signals with an intrinsic drive to minimize free energy,\\nallowing agents to naturally balance exploration and exploitation through a\\nunified Bayesian objective. By integrating Large Language Models as generative\\nworld models with AIF's principled decision-making framework, we can create\\nagents that learn efficiently from experience while remaining aligned with\\nhuman values. This synthesis offers a compelling path toward AI systems that\\ncan develop autonomously while adhering to both computational and physical\\nconstraints.\\n\", 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': '- Active Inference (AIF),   - Large Language Models (as generative world models),   - Bayesian objective for balancing exploration and exploitation,   - Free energy minimization, , -', 'benefits': \"- AIF provides a foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering.,   - AIF can bridge the grounded-agency gap by replacing external reward signals with an intrinsic drive to minimize free energy.,   - Integration of Large Language Models with AIF's decision-making framework can create agents that learn efficiently from experience while remaining aligned with human values.,   - This approach offers a path toward AI systems that can develop autonomously while adhering to computational and physical constraints., , -\", 'limitations': '- Current AI systems face significant scalability challenges due to reliance on high-quality training data and extensive human workforces for reward design.,   - The proposed \"Era of Experience\" still depends on extensive human engineering of reward functions, shifting the bottleneck from data curation to reward curation., , -', 'gaps': '- The grounded-agency gap: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances.'}\n",
      "{'ref_no': '2508.05614v1', 'title': 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks', 'authors': 'Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang', 'url': 'http://arxiv.org/abs/2508.05614v1', 'abstract': '  Large language models excel at abstract reasoning but their capacity for\\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\\ncomprehensive framework for evaluating how language models reason about\\nphysical interactions, tool usage, and multi-agent coordination in embodied\\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\\ncollaboration directives, OmniEAR requires agents to dynamically acquire\\ncapabilities and autonomously determine coordination strategies based on task\\ndemands. Through text-based environment representation, we model continuous\\nphysical properties and complex spatial relationships across 1,500 scenarios\\nspanning household and industrial domains. Our systematic evaluation reveals\\nsevere performance degradation when models must reason from constraints: while\\nachieving 85-96% success with explicit instructions, performance drops to\\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\\ntasks showing over 50% failure rates. Surprisingly, complete environmental\\ninformation degrades coordination performance, indicating models cannot filter\\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\\nfundamental architectural limitations. These findings demonstrate that embodied\\nreasoning poses fundamentally different challenges than current models can\\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\\nadvancing embodied AI systems. Our code and data are included in the\\nsupplementary materials and will be open-sourced upon acceptance.\\n', 'comment': 'Project Page: https://zju-real.github.io/OmniEmbodied Code:\\n  https://github.com/ZJU-REAL/OmniEmbodied', 'journal_ref': None, 'doi': None, 'tools_tech_method': '- OmniEAR framework,   - Text-based environment representation,   - Fine-tuning of language models,   - Evaluation across 1,500 scenarios spanning household and industrial domains, , -', 'benefits': \"- Comprehensive evaluation of language models' reasoning about physical interactions, tool usage, and multi-agent coordination,   - Dynamic acquisition of capabilities and autonomous determination of coordination strategies,   - Systematic evaluation revealing performance insights,   - Establishment of a rigorous benchmark for embodied AI systems,   - Open-sourcing of code and data, , -\", 'limitations': '- Severe performance degradation when models must reason from constraints,   - Performance drops significantly for tool reasoning and implicit collaboration tasks,   - High failure rates for compound tasks,   - Degraded coordination performance with complete environmental information,   - Minimal multi-agent gains from fine-tuning, , -', 'gaps': '- The need for improved models that can better handle embodied reasoning tasks,   - The challenge of filtering task-relevant constraints from complete environmental information,   - Fundamental architectural limitations in current models for multi-agent tasks,   - The necessity for further advancements in embodied AI systems to address the unique challenges posed by embodied reasoning'}\n",
      "{'ref_no': '2508.05519v1', 'title': 'Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study\\n  of AI-Assisted vs. Traditional Methods', 'authors': 'Matthew Purri, Amit Patel, Erik Deurrell', 'url': 'http://arxiv.org/abs/2508.05519v1', 'abstract': '  Clinical trial data cleaning represents a critical bottleneck in drug\\ndevelopment, with manual review processes struggling to manage exponentially\\nincreasing data volumes and complexity. This paper presents Octozi, an\\nartificial intelligence-assisted platform that combines large language models\\nwith domain-specific heuristics to transform clinical data review. In a\\ncontrolled experimental study with experienced clinical reviewers (n=10), we\\ndemonstrate that AI assistance increased data cleaning throughput by 6.03-fold\\nwhile simultaneously decreasing cleaning errors from 54.67% to 8.48% (a\\n6.44-fold improvement). Crucially, the system reduced false positive queries by\\n15.48-fold, minimizing unnecessary site burden. These improvements were\\nconsistent across reviewers regardless of experience level, suggesting broad\\napplicability. Our findings indicate that AI-assisted approaches can address\\nfundamental inefficiencies in clinical trial operations, potentially\\naccelerating drug development timelines and reducing costs while maintaining\\nregulatory compliance. This work establishes a framework for integrating AI\\ninto safety-critical clinical workflows and demonstrates the transformative\\npotential of human-AI collaboration in pharmaceutical clinical trials.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': '- Artificial Intelligence (AI)-assisted platform: Octozi,   - Large language models,   - Domain-specific heuristics,   - Controlled experimental study with experienced clinical reviewers (n=10), , -', 'benefits': '- Increased data cleaning throughput by 6.03-fold,   - Decreased cleaning errors from 54.67% to 8.48% (a 6.44-fold improvement),   - Reduced false positive queries by 15.48-fold, minimizing unnecessary site burden,   - Consistent improvements across reviewers regardless of experience level,   - Potential to accelerate drug development timelines and reduce costs,   - Maintained regulatory compliance, , -', 'limitations': \"- The study's sample size is relatively small (n=10), which may not represent the entire population of clinical reviewers.,   - The abstract does not mention any potential drawbacks or challenges of implementing the AI-assisted platform in real-world settings.,   - The long-term effects and adaptability of the AI system to evolving clinical trial data and protocols are not discussed., , -\", 'gaps': \"- The abstract does not provide information on how the AI system handles different types of clinical trial data or its adaptability to various trial phases.,   - There is no mention of how the AI system ensures data privacy and security, which is crucial in clinical trials.,   - The abstract does not discuss the potential for bias in the AI system and how it is addressed.,   - Further research is needed to evaluate the system's performance in diverse clinical trial settings and with larger groups of reviewers.\"}\n",
      "{'ref_no': '2508.05469v1', 'title': \"Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond\\n  Vibes\", 'authors': 'Zachary Robertson, Sanmi Koyejo', 'url': 'http://arxiv.org/abs/2508.05469v1', 'abstract': '  We develop mechanisms for evaluating AI systems without ground truth by\\nexploiting a connection between gaming resistance and output quality. The data\\nprocessing inequality ensures post-hoc attempts to game a metric degrades both\\ninformation content and task performance. We prove that f-mutual information\\nmeasures are the unique gaming resistant mechanisms under natural conditions,\\nwith the overseer acting as an agent. While Shannon mutual information faces\\nexponential sample complexity, bounded measures like total variation distance\\nremain tractable. Empirically, across ten domains from translation to peer\\nreview, all information-theoretic mechanisms achieve perfect discrimination (d\\n> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit\\nsystematic evaluation inversion, preferring fabricated content over accurate\\nsummaries. Our mechanisms show 10-100x better robustness to adversarial\\nmanipulation than current practices. We also find performance follows an\\ninverted-U curve with compression ratio, peaking at 10:1 where agent responses\\nexhibit optimal information diversity (3 effective dimensions), giving a\\nbias-variance perspective on when our approach is expected to be most\\neffective.\\n', 'comment': '13 pages', 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n",
      "{'ref_no': '2508.05464v1', 'title': 'Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?', 'authors': 'Matteo Prandi, Vincenzo Suriani, Federico Pierucci, Marcello Galisai, Daniele Nardi, Piercosma Bisconti', 'url': 'http://arxiv.org/abs/2508.05464v1', 'abstract': '  The rapid advancement of General Purpose AI (GPAI) models necessitates robust\\nevaluation frameworks, especially with emerging regulations like the EU AI Act\\nand its associated Code of Practice (CoP). Current AI evaluation practices\\ndepend heavily on established benchmarks, but these tools were not designed to\\nmeasure the systemic risks that are the focus of the new regulatory landscape.\\nThis research addresses the urgent need to quantify this \"benchmark-regulation\\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\\nwidely-used benchmarks against the EU AI Act\\'s taxonomy of model capabilities\\nand propensities. Our findings reveal a profound misalignment: the evaluation\\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\\nbias\" (28.9%), while critical functional capabilities are dangerously\\nneglected. Crucially, capabilities central to loss-of-control scenarios,\\nincluding evading human oversight, self-replication, and autonomous AI\\ndevelopment, receive zero coverage in the entire benchmark corpus. This\\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\\nprovides the first comprehensive, quantitative analysis of this gap, offering\\ncritical insights for policymakers to refine the CoP and for developers to\\nbuild the next generation of evaluation tools, ultimately fostering safer and\\nmore compliant AI.\\n', 'comment': None, 'journal_ref': None, 'doi': None, 'tools_tech_method': 'Not available', 'benefits': 'Not available', 'limitations': 'Not available', 'gaps': 'Not available'}\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "\n",
    "# Set up Mistral API key\n",
    "if \"MISTRAL_API_KEY\" not in os.environ:\n",
    "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n",
    "\n",
    "# Initialize Mistral API client\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-large-latest\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "# Function to analyze the abstract using Mistral\n",
    "def analyze_abstract_with_mistral(abstract):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following information from the abstract:\n",
    "    1. Tools/Tech/Methods\n",
    "    2. Benefits\n",
    "    3. Limitations\n",
    "    4. Gaps\n",
    "\n",
    "    Abstract: {abstract}\n",
    "\n",
    "    Please provide the information in the following format:\n",
    "    - Tools/Tech/Methods: <list of tools, methods, or technologies used>\n",
    "    - Benefits: <list of benefits>\n",
    "    - Limitations: <list of limitations>\n",
    "    - Gaps: <list of research gaps>\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        (\"system\", \"You are a helpful assistant that extracts methods, benefits, limitations, and gaps from academic abstracts.\"),\n",
    "        (\"human\", prompt)\n",
    "    ]\n",
    "    \n",
    "    ai_msg = llm.invoke(messages)\n",
    "    \n",
    "    # Debugging: Print the raw response from Mistral for analysis\n",
    "    print(f\"Raw Mistral Response: {ai_msg}\")\n",
    "    \n",
    "    # Check if Mistral returned valid response\n",
    "    if hasattr(ai_msg, 'content') and ai_msg.content.strip():\n",
    "        result_text = ai_msg.content\n",
    "    else:\n",
    "        result_text = \"Not available\"  # If Mistral did not generate any response\n",
    "    \n",
    "    # Parse the response and extract structured data\n",
    "    return parse_analysis(result_text)\n",
    "\n",
    "\n",
    "def parse_analysis(response_text):\n",
    "    # Initialize default analysis structure\n",
    "    analysis = {\n",
    "        'tools_tech_method': 'Not available',\n",
    "        'benefits': 'Not available',\n",
    "        'limitations': 'Not available',\n",
    "        'gaps': 'Not available'\n",
    "    }\n",
    "\n",
    "    # Define regex patterns for each section\n",
    "    pattern_tools_tech = r\"\\*\\*Tools/Tech/Methods\\*\\*:(.*?)\\*\\*Benefits\\*\\*\"\n",
    "    pattern_benefits = r\"\\*\\*Benefits\\*\\*:(.*?)\\*\\*Limitations\\*\\*\"\n",
    "    pattern_limitations = r\"\\*\\*Limitations\\*\\*:(.*?)\\*\\*Gaps\\*\\*\"\n",
    "    pattern_gaps = r\"\\*\\*Gaps\\*\\*:(.*)\"\n",
    "\n",
    "    # Extracting tools, benefits, limitations, gaps using regex\n",
    "    tools_match = re.search(pattern_tools_tech, response_text, re.DOTALL)\n",
    "    benefits_match = re.search(pattern_benefits, response_text, re.DOTALL)\n",
    "    limitations_match = re.search(pattern_limitations, response_text, re.DOTALL)\n",
    "    gaps_match = re.search(pattern_gaps, response_text, re.DOTALL)\n",
    "\n",
    "    if tools_match:\n",
    "        analysis['tools_tech_method'] = tools_match.group(1).strip().replace(\"\\n\", \", \").strip()\n",
    "    if benefits_match:\n",
    "        analysis['benefits'] = benefits_match.group(1).strip().replace(\"\\n\", \", \").strip()\n",
    "    if limitations_match:\n",
    "        analysis['limitations'] = limitations_match.group(1).strip().replace(\"\\n\", \", \").strip()\n",
    "    if gaps_match:\n",
    "        analysis['gaps'] = gaps_match.group(1).strip().replace(\"\\n\", \", \").strip()\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Function to fetch papers from ArXiv API\n",
    "def fetch_papers_from_arxiv(keyword, topic, max_results=5, start=0):\n",
    "    search_query = f\"{topic} AND {keyword}\"\n",
    "    api_url = f\"http://export.arxiv.org/api/query?search_query={search_query}&start={start}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "    \n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data from ArXiv API. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        url = entry.find('{http://www.w3.org/2005/Atom}id').text\n",
    "        abstract = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        \n",
    "        authors = [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')]\n",
    "        \n",
    "        comment = entry.find('{http://arxiv.org/schemas/atom}comment')\n",
    "        journal_ref = entry.find('{http://arxiv.org/schemas/atom}journal_ref')\n",
    "        doi = entry.find('{http://arxiv.org/schemas/atom}doi')\n",
    "        \n",
    "        # Use Mistral API to analyze the abstract\n",
    "        analysis = analyze_abstract_with_mistral(abstract)\n",
    "        \n",
    "        papers.append({\n",
    "            'ref_no': url.split('/')[-1],\n",
    "            'title': title,\n",
    "            'authors': ', '.join(authors),\n",
    "            'url': url,\n",
    "            'abstract': abstract,\n",
    "            'comment': comment.text if comment is not None else None,\n",
    "            'journal_ref': journal_ref.text if journal_ref is not None else None,\n",
    "            'doi': doi.text if doi is not None else None,\n",
    "            'tools_tech_method': analysis['tools_tech_method'],\n",
    "            'benefits': analysis['benefits'],\n",
    "            'limitations': analysis['limitations'],\n",
    "            'gaps': analysis['gaps']\n",
    "        })\n",
    "    \n",
    "    return papers\n",
    "\n",
    "\n",
    "# Function to save the data into an Excel file\n",
    "def save_to_excel(papers, filename=\"literature_survey.xlsx\"):\n",
    "    if papers:\n",
    "        df = pd.DataFrame(papers)\n",
    "        df.to_excel(filename, index=False)\n",
    "        print(f\"Excel file saved as {filename}\")\n",
    "    else:\n",
    "        print(\"No papers found to save.\")\n",
    "\n",
    "\n",
    "# Example usage: fetching papers and saving to Excel\n",
    "papers = fetch_papers_from_arxiv(\"AI\", \"Systems\")\n",
    "save_to_excel(papers)\n",
    "\n",
    "# Print the first paper for verification\n",
    "for paper in papers:\n",
    "    print(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e40d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
